\section{Mixing and hitting times for Markov chains (Perla Sousi)}

Mixing times for Markov chains is an active area of research in modern probability and it lies at the interface of mathematics, statistical physics and theoretical computer science. The mixing time of a Markov chain is defined to be the time it takes to come close to equilibrium. There is a variety of techniques used to estimate mixing times, coming from probability, representation theory and spectral theory. In this mini course I will focus on probabilistic techniques and in particular, I will present some recent results (see references below) on connections between mixing times and hitting times of large sets. 

Website: \url{https://www.math.ubc.ca/Links/OOPS/abs_Sousi.php}

\subsection*{2020/6/15 Lecture 1}

I'll cover results from 3 papers.
\begin{enumerate}
\item
Equivalence (up to constants) between mixing times and hitting times of large sets
\item
Hitting times: comparison for different sizes of sets
\item
Refined mixing and hitting equivalence
\end{enumerate}•

Let $X$ be an irreducible Markov chain in a finite state space $S$. (You can go from any state to any other state in a finite number of steps with positive probability.) Let $P$ be the transition matrix of $X$. 
Let $P^t(i,j)=\Pj_i(X_t=j)$ for all $i,j\in S$ (starting at $i$, get to $j$ in $t$ steps. 

There exists an invariant distribution $\pi$, $\pi=\pi P$. 
If $X$ is also aperiodic, then $P^t(x,y)\to \pi(y)$ as $t\to \iy$, forall $x,y$. 

We use the total variation distance. Let $\mu$ and $\nu$ be 2 probability distributions on $S$. Let 
\begin{align*}
\ve{\mu - \nu}_{TV}=\max_{A\sub S}|\mu(A)-\nu(A)|.
\end{align*}•
Let
\begin{align*}
d(t) &= \max_x\ve{P^t(x,\cdot)-\pi}_{TV}.
\end{align*}•
(Take over the worst starting state.). For all $\ep\in (0,1)$, 
\begin{align*}
t_{\text{mix}} &= \min \set{t\ge 0}{d(t)\le \ep}.
\end{align*}•
Define $t_{\text{mix}}\prc4$. 

$X$ is called \vocab{reversible} if from the stationary distribution, running the Markov chain forwards or backwards in time is indistinguishable: for all $x,y$,
\begin{align*}
\pi(x)P(x,y) &= \pi(y)P(y,x).
\end{align*}•
I'll mostly talk about the reversible case.

We always consider the lazy verion of the chain. The lazy version of $X$: either stay with probability $\rc 2$, or choose a state with respect to the transition matrix. So $P_L = \fc{P+I}2$.

\begin{thm}[Oliveira, Peres-S, 2012]\label{t:mc1}
For all $\al<\rc2$, there exist positive constants $c_\al$ and $c_\al'$ such that for all reversible  lazy Markov chains,
\begin{align*}
c_\al t_H(\al)&\le t_{\text{mix}} \le c_\al' t_H(\al).
\end{align*}•
\end{thm}
I will also write this as $t_{\max}\asymp_\al t_H(\al)$. 
Here $t_H(\al)$ is the maximum hitting time of sets of size at least $\al$.
\begin{align}
t_H(\al) &= \max_{x,A:\pi(A)\ge \al} \E_x[\tau_A].
\end{align}•
The lower inequality is the easy version: if we don't hit a big set, we cannot be mixed. The upper inequality is nontrivial, and where all of the work goes.

\begin{proof}[Proof of Theorem~\ref{t:mc1}, lower bound]
We show $t_{\text{mix}}\ge c_\al t_H(\al)$. We take $\al=\rc 8$, to make calculations easier.

We show $t:=t_{\text{mix}}\prc 16\le 3t_{\text{mix}}$. Use submultiplicative (see notes on webpage), lose a factor of 2.

For all $x, A$, $P^t(x,A)\ge \pi(A)-\rc{16}$. Take $A$ with $\pi(A)\ge \rc 8$, then $P^t(x,A)\ge \rc{16}$ for all $x$. 

So $\tau_A \le t\cdot \text{Geo}\prc{16}$: if we haven't hit $A$ after time $t$, we have $\ge \rc{16}$ chance of hitting it in the next $t$ steps, and so on. 
%perform an independent experiment
Hence $\max_x \E_x[\tau_A]\le 16t$. 
\end{proof}
(The lower bound doesn't need reversibility.)
Related result: Aldous in 1982 showed that for all reversible lazy MC's,
\begin{align*}
t_{\text{mix}} \asymp \max_{x,A}\pi(A) \E_x[\tau_A]
\end{align*}•
(He showed it for continuous MC, but this is similar to a lazy MC.)
%hitting time for lazy and nonlazy MC a factor 1/2 apart.
%balance out, small times big.
Where is this maximum actually attained? The theorem says it is attained at large sets.

\begin{rem}
Reversibility is essential!
\end{rem}
\begin{exr}
Consider a biased RW on $\Z_n$ (with laziness). With probability $\rc 3$ move to $i-1$, with probability $\fc23$ move to $i+1$, and take $P_L = \fc{P+I}2$. 

Show $t_{\text{mix}}\asymp n^2$ and for all $\al$, $t_H(\al)\asymp n$. 

This is a counterexample when the chain is not reversible.
\end{exr}
%constants explode as constant go to 1/2
\begin{rem}
If $\al>\rc 2$, then the theorem is false. The constants explose as the constant $\nearrow \rc2$.
\end{rem}

\begin{exr}
Consider $K_n$ and $K_n$ joined by an edge.
Show that $t_{\text{mix}}\asymp n^2$ and $t_H(\al)\asymp n$ if $\al>\rc2$.
\end{exr}
%another notion of mixing.
%total variation before
\begin{df}[Mixing at a geometric time]
Let $Z_t$ be a geometric r.v. of parameter $\rc t$ taking values in $\{1,\ldots\}$ and independent of $X$. 
Define $d_G(t)=\max_x\ve{P_x(X_{Z_t}=\cdot) - \pi}_{TV}$ and $t_G=\min\set{t\ge 0}{d_G(t)\le \rc 4}$ as the \vocab{geometric mixing time}.
\end{df}
%tails of hitting times of large sets
%random time expectation order t.
%If instead of $Z_t$ being geometric, we consider a uniform random variable
\begin{rem}
If instead of geometric, we take $U_t$ to be uniform on $\{1,\ldots,t\}$, then this gives rise to \vocab{Cesaro mixing time}.
\end{rem}
\begin{exr}
Show that $d_G(t)$ is decreasing in $t$.
\end{exr}
This is not true of the Cesaro mixing time.
\begin{thm}\label{t:mc2}
For all (lazy) reversible chains, $t_G\asymp t_{\text{mix}}$.
\end{thm}
The ideas come from Aldous, and Lov\'asz and Winkler.
\begin{thm}\label{t:mc3}
For all chains (not necessarily lazy), $t_G\asymp_\al t_H(\al)$ for all $\al<\rc 2$.
\end{thm}
\begin{proof}[Proof of Theorem~\ref{t:mc1}, upper bound]
Immediate from Theorems~\ref{t:mc2} amd~\ref{t:mc3}.
\end{proof}
\begin{proof}[Proof of Theorem~\ref{t:mc3}]
$t_G\gtrsim_\al t_H(\al)$ is easy. 

We prove $t_G\lesssim_\al t_H(\al)$. Take $\al=\rc8$.

Let $t<t_G$.
We want to find a set $B$  with $\pi(B)\ge \rc 8$ such that $\max_x\E_x[\tau_B]\ge \te t$ for some positive constant $\te$. 

$t<t_G$ means that there exist $z,A$ such that $\Pj_z(X_{Z_t}\in A)< \pi(A)-\rc 4$.
We automatically get that $\pi(A)>\rc 4$. This is not the set $B$, but will be used to define $B$.

Define $B=\set{y}{\Pj_y(X_{Z_t}\in A)\ge \pi(A)-\rc 8}$. 
%A lot of starting points bad for $A$ in this sense. 
\begin{clm*}
$\pi(B)>\rc 8$
\end{clm*}
%$A$ is large and $\pi$ is stationary.
$\pi = \pi P$ implies
\begin{align*}
\pi(A) &= \ub{\sum_{y\in B} \pi(y) \ub{\Pj_y(X_{Z_t}\in A)}{\le 1}}{\le \pi(B)} + \ub{\sum_{y\in B^c} \pi(y)}{\le1} \ub{\Pj_y(X_{Z_t}\in A)}{\le \pi(A)-\rc 8}\\
\pi(A) &\le \pi(B) + \pi(A)-\rc 8 \\
\implies 
\pi(B) &\ge \rc 8,
\end{align*}•
%stnry + independent time
proving the claim.

We will prove that assuming $\E_z[\tau_B]\le \te t$ for a suitable constant $\te$ leads to a contradiction.
$B$ is the set of points, such that starting from there, we have substantial probability of hitting $A$. $z$ is bad starting state, the fact that we hit quickly from $z$ will contradict.

By Markov's inequality, 
\begin{align}\label{e:mc-mi}\Pj_z(\tau_B \ge 2\te Mt) &\le \rc{2M},
\end{align}
for $M\in \N$.
\begin{align*}
\Pj_z(X_{Z_t}\in A) 
&\ge \ub{\Pj_z(X_{Z_t}\in A| Z_t\ge \tau_B,\tau_B<2\te Mt)}{\ge \min_{y\in B} (X_{Z_t}\in A)}
\Pj_z(Z_t\ge \tau_B, \tau_B <2\te Mt)
\end{align*}•
by the memoryless property of $Z_t$ and strong Markov at $\tau_B$.
\begin{align*}
&\ge \pa{\pi(A) - \rc 8} \ub{\Pj_z(Z_t>2\te Mt, \tau_B<2\te Mt)}{\Pj_z(Z_t>2\te Mt) \Pj_z(\tau_B<2\te Mt)}\\
&\ge \pa{\pi(A)-\rc 8} \pa{1-\rc t}^{2\te M t}\pa{1-\rc{2M}}&\text{by~\eqref{e:mc-mi}}\\
&\ge \pa{\pi(A) - \rc 8}\pa{1-2\te M} \pa{1-\rc{2M}}&2\te Mt>1.
\end{align*}•
Choosing $\te = \rc{4M^2}$ gives $\Pj_z(X_{Z_t}\in A)\ge \pa{\pi(A)-\rc 8} \pa{1-\rc{2M}}^2$. 
Taking $M$ large enough shows $\Pj_z(X_{Z_t}\in A)>\pi(A)-\rc 4$ which is a contradiction (to the fact that $z$ is a bad starting point for $A$.
\end{proof}
The idea of geometric mixing is due to Oded Schramm.

%lower bound >\pi(A)-\rc 4$
Idea of Theorem~\ref{t:mc2}: Let 
\begin{align*}
t_{\text{stop}} &= \max_x\min \set{\E_x[\La_x]}{\La_x \text{ is a randomised stopping time s.t. }\Pj_x(X_{\La_x}=\cdot)=\pi(\cdot)}.
\end{align*}•
%can include extra randomized.
Take the worst starting state $x$, find a randomised stopping time which achieves stationarity. Find the stopping time with minimum expectation. It is not at all obvious that it exists. It's clear that there is one achieving stationarity, it's not clear that the minimum is attained.

The filling rule: The construction goes back to Baxter and Chacon, '76, used by Aldous, Lov\'asz-Winkler. 

The easy direction is $t_{\text{stop}}\le 8t_{\text{mix}}$. The hard direction is to show that $t_{\text{stop}}\gtrsim t_{\text{mix}}$. 
\begin{exr}
Prove that for reversible chains, $t_{\text{stop}}\le 8t_{\text{mix}}$. 

Hint: use separation distance to define an appropriate stopping time.
\end{exr}
%Lovasz and Winkler, mixing times.
%mixing time of the continuous-time version of a discrete time chain is up to a constant factor the same as that of the lazy one.
%cts always aperiodic


\subsection*{2020/6/15 Lecture 2}

We consider removing the reversibility assumption. Recall that the mixing time for the biased random walk on $\Z_n$ satisfies $t_{\text{mix}}\asymp n^2$, $t_H(\al)\asymp n$ for all $\al$, $\max_{x,y} \E_x[\tau_y]\asymp n$.
%instead of specific set, hit moving set

Instead of hitting a specific set, we can ask for hitting a moving set.
For $\al\in (0,1)$ define $\cA(\al) = \set{A=(A_t)_{t\ge 0}}{\pi(A_t)\ge \al, \forall t\ge 0}$ and define also $\tau_A = \inf\set{t\ge 0}{X_t\in A_t}$ for $A=(A_t)_{t\ge 0}$.

\begin{thm}[Winkler-S]
Fix $\al<\rc 2$. Then there exists two positive constants $c_\al$ and $c_\al'$ such that for all irreducible finite Markov chains if
\begin{align*}
t_{\text{mov}}&= \sup_{x,A\in \cA(\al)} \E_x[\tau_A],
\end{align*}•
then 
\begin{align*}
c_\al t_{\text{mov}}(\al)\le t_{\text{mix}} \le c_\al't_{\text{mov}}.
\end{align*}•
\end{thm}
The chain doesn't have to be lazy: If it's periodic, then both $t_{\text{mix}}$ and $t_{\text{mov}}$ are infinite.
%does not go through geometric mixing times
%first hit B then A

The second thing I'll discuss is what happens when $\al=\rc2$. 
\begin{thm}[Griffiths, Kang, Oliveira, Patel]\label{t:mc2-1}
Let $0<\al<\be\le \rc2$. Then for every irreducible finite Markov chain, 
\begin{align*}
t_H(\al) \le t_H(\be) + \pa{\rc\al - 1}t_H(1-\be) \le \rc\al t_H(\be).
\end{align*}•
\end{thm}
The only assumption is irreducible; reversibility is not needed. 
Recall that $t_H(\al) = \max_{x,A:\pi(A)\ge \al}\E_x[\tau_A]$.
The theorem compares the hitting times of sets of size at most $\al$ to hitting times of sets of size at most $\be$. Note $t_H(\be) \le t_H(\al)$. Note $1-\be \ge \be$, hence the RHS inequality.
\begin{rem}
These 2 inequalities are sharp, i.e., for all $0<\al<\be \le \rc 2$ there exists an irreducible finite Markov chain for which all three terms are equal. In fact, we can find a 3-state Markov chain.

The condition $\be \le \rc2$ is sharp. $\be=\rc2$ is a boundary case, in the sense that for all $\be>\rc2$, there exists a class of irreducible finite MC's such that $t_H(\al)/t_H(\be)$ can be made arbitrarily large.
%I'll give the proof, but first explain how it completes the previous theorem. 
\end{rem}
We'll need the following lemma to prove the theorem.
\begin{lem}\label{l:mc2-1}
For an irreducible finite MC with state space $S$, for any nonempty $A, B\subeq S$ define
\begin{align*}
d^+(A,B) &= \max_{x\in A} \E_x[\tau_B]\\
d^-(A,B) &= \min_{x\in A} \E_x[\tau_B].
\end{align*}•
Then 
\begin{align*}
\pi(A) &\le \fc{d^+(A,B)}{d^+(A,B)+d^-(B,A)}. 
\end{align*}•
\end{lem}
First I give a non-rigorous explanation. We want to show $\pi(A)\cdot (d^+(A,B)+d^-(B,A)) \le d^+(A,B)$. Let $x$ be such that $\E_x[\tau_B]=d^+(A,B)$. We want to show 
\begin{align*}
\pi(A) (\E_x [\tau_B] + d^-(A,B))\le d^+(A,B).
\end{align*}•
Because the Markov chain is irreducible and has an invariant distribution, by the ergodic theorem, by time $t$ the chain visits the set $A$ $\pi(A)t$ times. Define $\tau_{B,A}=\min\set{t\ge \tau_B}{X_t\in A}$. By time $\tau_{B,A}$, ``the chain spends time $\pi(A)\E_x[\tau_{B,A}]$."
%only time 0 to tau_B. no more visits to A.
Also the time spent is $\le \E_x[\tau_B]$ (after $\tau_B$ there are no more visits to $A$), so 
\begin{align*}
\pi(A) \ub{\E_x[\tau_{B,A}]}{\E_x[\tau_B]+d^-(B,A)} &\le \E_x[\tau_B] = d^+(A,B). 
\end{align*}•
%why after \tau_B there are no more visits to A
%don't count first time in A
%tau_B, tau_A-1

%\tau_{B,A} was the first time after tau_B that the chain visits A. So in the time interval [\tau_B, \tau_{B,A}) the chain cannot visit $A$ by definition

\begin{proof}[Proof of Theorem~\ref{t:mc2-1}]
Fix $x$ and a set $A$ with $\pi(A)\ge \al$. We want to show $\E_x[\tau_A]\le \tau_H(\be) + \pa{\rc \al -1}t_H(1-\be)$. 

The idea is the same: define the right set $B$ and first try to hit that set. We want to define a set $B$ with $\pi(B)\ge \be$, so that we first wait to hit $B$ and then starting from there the time to hit $A$ is controlled the second term.

Define $B=\set{y}{\E_y[\tau_A]\le \pa{\rc \al - 1}t_H(1-\be)}$. If we show $\pi(B)\ge \be$, then we are done, because
\begin{align*}
\E_x[\tau_A] &\le \E_x [\tau_B] + \max_{y\in B}\E_y[\tau_A] \le \tau_H(\be) + \pa{\rc\al-1}t_H(1-\be).
\end{align*}•

We claim that $\pi(B)\ge \be$. 
%could take longer - hit A on the way to B.

Suppose not, i.e., $\pi(B)<\be$ and let $C=B^c$. Then $\pi(C)>1-\be$.
\begin{align*}
\pi(A) &\le \fc{d^+(A,C)}{d^+(A,C)+d^-(C,A)}
\end{align*}•
using Lemma. Now $d^+(A,C) \le t_H(1-\be)$ and $d^-(C,A)> \pa{\rc\al-1} t_H(1-\be)$. Substituting,
\begin{align*}
\pi(A) &< \fc{1}{1+\rc \al - 1}=\al.
\end{align*}•
which is a contradiction.
\end{proof}

We need the following to prove Lemma~\ref{l:mc2-1}.
\begin{lem}[Lemma~\ref{l:mc2-2}]
Let $X$ be an irreducible finite Markov chain with values in $S$. Let $\mu$ be a probability distribution and $\tau$ a stopping time such that $\Pj_\mu(X_\tau=x)=\mu(x)$ for all $x$.
Then 
\begin{align*}
\E_\mu \ba{\sumz i{\tau-1} 1(X_i\in A)} &= \pi(A) \E_\mu[\tau]
\end{align*}•
for all $A\subeq S$. 
\end{lem}
\begin{proof}
Exercise.

Hint: Let $\nu(x) = \E_{x_0}\ba{\sumz i{\tau_{x_0}^+-1} 1(X_i=x)}$. Then $\nu = \nu P$. Normalizing gives the invariant distribution $\pi$. Here $\tau_{x_0}^+ = \min\set{t\ge 1}{X_t=x_0}$. 

Instead of starting from a fixed point $x_0$, we run from distribution $\mu$, and we run until a stopping time with the given property. 
\end{proof}

%positive recurrence
%how to prove invariant distribution?
\begin{proof}[Proof of Lemma~\ref{l:mc2-1}]
Define $$\tau_{B,A} = \min\set{t\ge \tau_B}{X_t\in A}$$
%want to get to setting of lemma 2. 
and an auxiliary Markov chain with transition matrix $Q$ given by 
\begin{align*}
Q(x,y) &= \Pj_x(X_{\tau_{B,A}}=y), \quad x,y\in A.
\end{align*}•
This is a finite irreducible Markov chain, so it possesses an invariant distribution $\mu$. Let $\nu(y)=\Pj_\mu(X_{\tau_B}=y)$, $y\in B$.
Count the number of visits to $A$ up until $\tau_{B,A}$ starting from $\mu$. Call $\tau=\tau_{B,A}$.
\begin{align*}
\E_\mu\ba{\sumz i{\tau-1} 1(X_i\in A)}&\le \E_\mu[\tau_B].
\end{align*}•
Because $\mu$ is invariant for $Q$, $\Pj_\mu(X_\tau=x)=\mu(x)$ for all $x$.
So the conditions of Lemma 2 are satisfied, and
\begin{align*}
\E_\mu\ba{\sumz i{\tau-1} 1(X_i\in A)}&= \pi(A) \E_\mu[\tau] = \pi(A) (\E_\mu[\tau_B]+\E_\nu[\tau_A]). 
%first have to hit B. Then want to hit A. dist of first hitting time 
\end{align*}•
%support of Q is not all of A, but is irred
%B can be the boundary of a bigger box than A and the domain of the whole graph
So
\begin{align*}
\pi(A) (\E_\mu[\tau_B] + \E_\nu[\tau_A]) &\le \E_\mu[\tau_B]\\
\implies
\pi(A) \E_\nu [\tau_A] &\le (1-\pi(A))\E_\mu[\tau_B]. 
\end{align*}•
This now completes the proof, because $\pi(A) \le \fc{d^+(A,B)}{d^+(A,B) + d^-(B,A)}$, which implies
\begin{align*}
\pi(A) d^-(B,A) &\le (1-\pi(A))d^+(A,B)
\end{align*}•
and $\E_\nu[\tau_A]\ge d^-(A,B)$ and $\E_\mu[\tau_B]\le d^+(A,B)$ ($\nu$ is supported on $B$ and $\mu$ is supported on $A$). 
\end{proof}
%loosely speaking, \mu is the hitting distribution of $A$ at equilibrium, after a visit to $B$.
%heuristic: time spent in $A$ by time $\tau_{A,B}$
%\mu is precisely the initial distribution wrt which the heuristic is a rigorous statement.

\subsection*{2020/6/18 Lecture 3}

We give refined mixing and hitting relations.
For $f,g:S\to \R$, define
\begin{align*}
\an{f,g}_\pi &= \sum_{x\in S} f(x)g(x)\pi(x).
\end{align*}•
The spectral representation theorem tells us that if $P$ is reversible wrt the invariant distribution $\pi$, then there exist $(\la_j)_{j=1}^{|S|}$ eigenvalues with corresponding eigenvectors $(f_j)_{j=1}^{|S|}$ and $\la_1$ and $f_1=(1,\ldots, 1)$. Then 
\begin{align*}
\fc{P^t(x,y)}{\pi(y)} &= 1+ \sum_{j=2}^{|S|} \la_j^t f_j(x) f_j(y).
\end{align*}•
\begin{proof}[Proof idea]
Define $A(x,y) = \sfc{\pi(x)}{\pi(y)} P(x,y)$; it is symmetric. 
\end{proof}
If $P$ is reversible we write the eigenvalues in decreasing order
\begin{align*}
1=\la_1>\la_2\ge \cdots \ge \la_n\ge -1.
\end{align*}•
Define $\la_*=\max\set{|\la|}{\la\text{ is an eigenvalue of }P\text{ with }\la\ne 1}$, $\ga_*=1-\la_*$ the absolute spectral gap, and $\ga=1-\la_2$ the spectral gap.

If a chain is lazy, then all eigenvalues are non-negative, so $\ga_*=\ga$. 

\begin{df}
The \vocab{relaxation time} is $t_{\text{rel}} = \rc{\ga_*}$. 
For $f:S\to \R$, define 
\begin{align*}
\E_\pi[f] &= \sum_x f(x) \pi(x)\\
\Var_\pi(f) &= \E_\pi[(f-\E_\pi[f])^2].
\end{align*}•
\end{df}
The Poincar\'e inequality says that for $P$ reversible and lazy, and for all $f$, $t\ge 0$, 
\begin{align*}
\Var_\pi (P^t f) &\le e^{-2t/t_{\text{rel}}}\Var_\pi(f).
\end{align*}•
Here $Pf(x) = \sum_y P(x,y)f(y)$. 
Define for $\ep,\al\in (0,1)$
\begin{align*}
\text{hit}_\al(\ep)&= \min\set{t}{\max_{x,A:\pi(A)\ge \al}\Pj(\tau_A>t)\le \ep}.
\end{align*}•
(Take the worst starting state and large $A$ such that the probability of hitting after time $t$ is small.)

\begin{thm}[Basu-Hermon-Peres]
Let $X$ be a reversible and lazy Markov chain on finite $S$ with $P$ and $\pi$. Then for all $\ep\in (0,1)$,
\begin{align}
\label{e:mc3-1}
t_{\text{mix}}(2\ep) &\le \text{hit}_{1-\ep}(\ep) + \ce{2t_{\text{rel}} \log\pf{2}{\ep^3}}\\
\label{e:mc3-2}
t_{\text{mix}}(1-\ep) &\le \text{hit}_{1-\ep}(1-2\ep) + \ce{2t_{\text{rel}} \log\pf{8}{\ep^3}}
\end{align}•
\end{thm}
The paper gave a bound on $t_{\text{mix}}(\ep+\de)$; the proof is the same so for sake of exposition we just show the bound for $2\ep$. We show~\eqref{e:mc3-1} and leave~\eqref{e:mc3-2} as an exercise.
 %and get more precise

This is the hard part; they also give a lower bound of similar order. As usual, the argument for the lower bound that if you haven't hit a big set you haven't mixed.

\begin{rem}
Mixing happens in 2 stages: in the first one $(\text{hit}_{1-\ep}(\ep))$ we have to wait to escape some small set with high probability and in the second one we wait for $t_{\text{rel}}$ steps and use the Poincar\'e inequality.
\end{rem}

\begin{proof}[Proof of~\eqref{e:mc3-1}]
We want to show that if we wait for the RHS amount of time, we have mixed and the TV distance is at most $2\ep$. Set $t=\text{hit}_{1-\ep}(\ep)$ and $s=\ce{2t_{\text{rel}}\log\pf{2}{\ep^3}}$. We want to show that for all $x,A$,
\begin{align*}
|P^{t+s}(x,A) - \pi(A)|&\le 2\ep.
\end{align*}•
The idea is that we want to define an intermediate set $G$ such that we hit it whp (wp $1-\ep$) before time $t$ and conditional on hitting it by time $t$ we want to be close to $\pi(A)$ at time $t+s$ by at most $\ep$, i.e., 
\begin{align*}
|\Pj_x(X_{t+s}\in A|\tau_G\le \tau)-\pi(A)|&\le \ep.
\end{align*}•
%Once we hit $G$, we have good control 
If $\Pj_x(\tau_G>t)<\ep$, then we will be done, because 
\begin{align*}
|\Pj_x(X_{t+s}\in A)-\pi(A)|
&= |\Pj_x(X_{t+s}\in A|\tau_G\le t)\Pj_x(\tau_G\le t)
+ \Pj_x(X_{t+s}\in A|\tau_G> t)\Pj_x(\tau_G>t)-\pi(A)|\\
&\le |\Pj_x(X_{t+s}\in A|\tau_G\le t)-\pi(A)| + \Pj_x(\tau_G>t)\\
&\le \sup_{y\in G,r\ge s} |\Pj_y(X_s\in A)- \pi(A)| + \Pj_x(\tau_G>t).
\end{align*}•
Define $G=\set{y}{\sup_{r\ge s} |\Pj_y(X_r\in A)-\pi(A)|\le \ep}$. Then 
\begin{align*}
|\Pj_x(X_{t+s}\in A)-\pi(A)| &\le \ep + \Pj_x(\tau_G>t).
\end{align*}•
So it suffices to show that $\Pj_x(\tau_G>t)\le \ep$. 
It suffices to prove that $\pi(G)>1-\ep$, since $\text{hit}_{1-\ep}(\ep)$. 

We need to show $\pi(G)>1-\ep$. 
%when close to equilibrium, worst case time to become decorrelated

For $f:S\to \R$, define $f^*(x) = \sup_{k\ge 0}|P^{2k}f(x)|$. Let
\begin{align*}
f_t(x) &= P^t(x,A) - \pi(A) = P^t(1(A)-\pi(A))(x).
\end{align*}•
Then
\begin{align*}
f_t^*(x) &= \sup_{k\ge 0}|P^{2k}f_t(x)| = \sup_{k\ge 0} |P^{2k+t}(x,A)-\pi(A)|\\
(Pf_t)^*(x) &= \sup_{k\ge 0} |P^{2k} Pf_t(x)| = \sup_{k\ge 0}|P^{2k+t+1}(x,A)-\pi(A)|\\
G&= \set{y}{f_s^*(y), (Pf_s)^*(y)\le \ep}\\
\pi(G^c) &\le \pi(\set{y}{f_s^*(y)>\ep}) + \pi(\set{y}{(Pf_s)^*(y)>\ep})\\
&\le \fc{\E_\pi[(f_s^*)^2]}{\ep^2} + \fc{\E_\pi[((Pf_s)^*)^2]}{\ep^2}
\end{align*}•
by Markov's inequality.
%rephrased definition
We write for $p\in (1,\iy)$ $\ve{f}_p^p=\E_\pi[|f|^p]$. Then
\begin{align*}
\ve{Pf_s}_2^2 &\le \ve{f_s}_2^2 &P\text{ is a contraction}\\
&=\Var_\pi(P^s(1(A)))\\
&\le e^{-2s/t_{\text{rel}}} \Var_\pi(1(A)) & \text{Poincar\'e}\\
&=\fc{\ep^3}2 \pi(A)(1-\pi(A)) \le \fc{\ep^3}{8} &\text{substituting }s
\end{align*}•
We want to control the norm; we do this by the following.
\begin{thm}[Starr's maximal inequality]
Let $P$ be reversible wrt $\pi$ and $p\in (1,\iy)$. For all $f:S\to \R$, 
\begin{align*}
\ve{f^*}_p &\le \fc{p}{p-1} \ve{f}_p.
\end{align*}•
\end{thm}
This is reminiscent of Doob's maximal inequality; the $\fc{p}{p-1}$ comes from Doob's inequality applied to an appropriate martingale.

By Starr,
\begin{align*}
\ve{(Pf_s)^*}_2^2 &\le 4 \ve{Pf_s}_2^2 \le \fc{\ep^3}{2}
\end{align*}•
and 
\begin{align*}
\ve{f_s^*}_2^2 &\le 4 \ve{f_s}_2^2 \le \fc{\ep^3}2.
\end{align*}•
Plug into (***) to get $\pi(G^c)<\eph+\eph=\ep$. 
\end{proof}

\begin{proof}[Proof of Starr]
Recall $f^*(x) = \sup_{k\ge 0}|P^{2k}f(x)|$. Let $X$ be a MC with $X_0\sim \pi$. Then 
\begin{align*}
P^{2n} f(X_0) &= \E[f(X_{2n}|X_0)] \\
&= \E[\E[f(X_{2n})|X_n,X_0]|X_0] &\text{tower property}\\
&= \E[\E[f(X_{2n})|X_n]|X_0] &\text{Markov property}.
\end{align*}•
Set $R_n=\E[f(X_{2n})|X_n]$. The goal is to show that $R$ is a backwards martingale. In other words we will show that if $N$ is fixed, then $(R_{N-n})_{0\le n\le N}$ is a martingale. 
Since $X_0\sim \pi$ and $X$ is reversible, it follows that $(X_n,X_{n+1},\ldots, X_{2n}) \sim (X_n,X_{n-1},\ldots, X_0)$. So 
\begin{align*}
R_n &= \E[f(X_{2n})|X_n] = \E[f(X_0)|X_n]\\
&= \E[f(X_0)|X_n,X_{n+1},\ldots]&\text{Markov property}.
\end{align*}•
Set $\cF_n=\si(X_n,X_{n+1},\ldots)$ and fix $N\ge 0$. Then $(R_{N-n})_{0\le n\le N}$ is a martingale wrt $(\cF_n)$.
This is good news because we want to control the maximum of $R_n$.
\begin{align*}
\ve{\max_{0\le n\le N}R_n}_p
&=\ve{\max_{0\le n\le N} R_{N-n}}_p\\
&\le \fc p{p-1} \ve{R_0}_0 = \fc{p}{p-1}\ve{f(X_0)}_p = \fc{p}{p-1}\ve{f}_p &\text{Doob's $L_p$-inequality}.
\end{align*}•
What we want to bound is
\begin{align*}
\ab{\max_{0\le n\le N} P^{2n}f(X_0)} &= \max_{0\le n\le N} \E [R_n|X_0] &\le \E[\max_{0\le n\le N} R_n|X_0].
\end{align*}•
Conditional Jensen implies that
\begin{align*}
\ve{\max_{0\le n\le N} P^{2n}f(X_0)}_p &\le \ve{\max_{0\le n\le N}R_n}_p
\le \fc{p}{p-1}\ve{f_p}.
\end{align*}•
Letting $N\to \iy$ and using Monotone Convergence completes the proof.
\end{proof}
The goal was to find a martingale to apply Doob's maximal inequality. We found $R$. The last step is to use conditional Jensen, let $N\to \iy$ and use monotone convergence. %This is a powerful result because it relates the supremum over all $k\ge 0$
%some absolute value signs are missing at the end of the proof (DMI and later)
%time difference to hit big sets and small sets prop to t_{rel}
%consider hitting sets of size at most 1-\ep
%Starrs inequality works for nonreversible, but statment different
%The elastic force with which the bdoy resists the deforming force diminishes with time. THe time which is necessary for the resisting force to fall to 1/e of its original value is called after Maxwell the relaxation time
%William Hobbs, mechanics of formation of arcuate mountains, 1914.
