\section{Branching random walks: some recent results and open questions (Nina Gantert)}

 We give an introduction to branching random walks and their continuous counterpart, branching Brownian motion. We explain some recent results on the maximum of a branching random walk and its relation to point processes, as well as a connection with fragmentations. The focus will be on open questions.

Preparatory reading: Lyons and Peres, Probability on Trees and Networks \url{http://pages.iu.edu/~rdlyons/prbtree/prbtree.html}, Chapters 5.1 (Galton-Watson branching processes) and 13.8 (Tree-indexed random walks)

Further reading:
\begin{itemize}
\item
Zhan Shi, Branching Random Walks, \url{https://www.lpsm.paris/pageperso/zhan/brw.html}
\item
Julien Berestycki, Topics on Branching Brownian motion, \url{http://www.stats.ox.ac.uk/~berestyc/Articles/EBP18_v2.pdf}
\item
Ofer Zeitouni, Branching Random Walks and Gaussian Fields, \url{http://www.wisdom.weizmann.ac.il/~zeitouni/pdf/notesBRW.pdf}
\end{itemize}•

%Piotr Dyszewski: Branching random walks and stretched exponential tails
%
%We will consider a branching process with a spacial component on the real line. After birth each individual performs an independent step according a stretched exponential (or Weibull) law. We will give a detailed description of the asymptotic behaviour of the position of the rightmost particle. The talk is based on a ongoing project with Nina Gantert and Thomas H?felsauer.
%Sam Johnston: The extremal particles of branching Brownian motion

\subsection*{2020/6/1 Lecture 1}

There are two ingredients for a branching random walk (BRW):
\begin{enumerate}
\item
Offspring law $p(\cdot)$ with $\sumo k\iy p(k)=1$. Assume $\sumo k\iy kp(k)>1$. Often we assume $p(0)=0$ to simplify.
\item 
Displacement law: Random variable $X$ with $\Var(X)>0$. 
\end{enumerate}•
Then
\begin{enumerate}
\item
Start with one particle at 0, particles produce offspring according to $p(\cdot)$.
\item 
Offspring take displacements according to $X$ (all particles behave independently).
\end{enumerate}•
%graph, spa
How does this crowd of particles behave?

There is a continuous analogue, branching Brownian motion (BBM) (see talk on Friday).
It follows Brownian motion and after an exponential lifetime, it splits into 2 particles which continue to follow Brownian motion (and splitting after exponential lifetime, and so on).

We first consider BRW. I will look at it as a tree-indexed random walk in the following sense. Note the Browian motion and branching are independent.

First build a Galton-Watson (GW) process according to $p(\cdot)$. Then label edges with iid random variables distributed as $X$.  
%put iid random variables on the edge of the tree.

Let $D_n$ be the vertices in generation $n$. Then $|D_n|$ is the Galton-Watson process.

The position of a particle $v$ is the sum of the random variables (displacements) on the edges leading to the vertex $v$: 
\begin{align*}
S_v &= \sum_{e\in [0,v]} X_e
\end{align*}
%position of particle $v$.
Recall the following.
\begin{thm}
If $m:=\sumz k\iy kp(k)>1$ then $\Pj(T\text{ infinite})>0$.

Here $m$ is the reproduction number. %hear about in the news
\end{thm}
If $p(0)>0$, we can look at $\Pj^*[\cdot]  =\Pj[\cdot ||D_n|>0, \forall n]$. This event has positive probability.

The collection $(S_v)_{v\in D_n}$ are random variables which are not independent.

%I'll give some results and open questions.

There is a more general model: 
%simultaneous production of offspring and displacement
Particles produce ``offspring and displacements" at once according to some point process.

For example, the point process could be:
\begin{itemize}
\item
produce particles at position 1 and -1,
\item
produce 3 particles at positions 3,
\end{itemize}•
each with probability $\rc2$. There is dependence between siblings, and displacements are not independent of tree.

Many things I will say will generalize to the general model.

The first question I will address is the behavior of the rightmost particle:
\begin{align*}
M_n &=\sup_{v\in D_n} S_v.
\end{align*}•
This is the maximum of variables with are not independent. %I want to say how the cloud spreads in space.
Assume the exponential moment condition
\begin{align*}
\E[e^{\la X}] &<\iy
\end{align*}•
for some $\la>0$ and define a large deviation rate function
\begin{align*}
I(y) &= \sup_\la [\la y - \log \E[e^{\la X}]]
\end{align*}•
%l.d. rate
It is a fact that for $y>\E[X]$, 
$$\rc{n} \log \Pj[S_n\ge ny]\to -I(y).$$

You can prove one direction quite easily:
\begin{align*}
\Pj(S_n>ny) &\le \E[e^{\la S_n}]e^{-\la ny}\\
&= e^{-n I(y)}
\end{align*}•
by Chebyshev's inequality and optimizing the RHS over $\la$. %$\la=\ol \la$ optimal.
Define
\begin{align*}
x^* &= \sup \set{s\ge \E[X]}{I(s)\le \log m}.
\end{align*}•
We have the following old result.
\begin{thm}[Biggins, Hammersley, Kingman]
\begin{align*}
\fc{M_n}n &\to x^*,\quad \Pj^*\text{-a.s.}
\end{align*}•
\end{thm}
The method of proof is useful.
\begin{exr}
Assume the theorem.
\begin{enumerate}
\item
Let $X\stackrel d=N(0,1)$. Compute $x^*$.
\item 
Suppose $p(3)=1$, $\Pj[X=0]=\rc 2 = \Pj[X=1]$. Compute $x^*$. Do we have $\Pj[M_n=n,\forall n]>0$?
\item
Same as (ii) if $p(2)=1$.
\end{enumerate}•
\end{exr}
I'll give my favorite proof of the theorem.

\paragraph{Intuition:} At time $n$, we have $\approx m^n$ particles. For each $v\in D_n$, $\Pj(S_u\ge ny) \approx e^{-n I(y)}$. We have 2 competing effects: the probability decays exponentially but the number of particles increase exponentially. These effects shold balance: if
\begin{align*}
e^{-n I(y)} \ub{e^{n\log m}}{m^n} &=1
\end{align*}•
then $y=x^*$.

\begin{proof}
\begin{enumerate}
\item
First moment method: This will give an upper bound. %for the speed
\begin{align*}
\Pj[M_n \ge ny] &\le \E\ba{\sum_{v\in D_n} I_{S_v\ge ny}}\\
&= \ub{\E [|\De_n|]}{m^n}\ub{\Pj[S_n\ge ny]}{\le e^{-nI(y)}}.
\end{align*}•
using the independence assumption.
Hence, if $I(y) > \log m$, then 
\begin{align*}
\sum_u \Pj[M_u \ge ny] &<\iy\\
\implies \limsup \fc{M_n}n&\le y.
\end{align*}
\item 
Embedded tree: Assume $y<x^*$. Choose $\ep>0$ such that $I(y)-2\ep<\log m$. Then using the lower bound
\begin{align*}
\Pj[S_k\ge ky] &\ge e^{-k(I(y)-\ep)}
\end{align*}•
for $k\ge k_0(\ep)$. I don't have strict inequality $e^{-kI(y)}$, I have an $\ep$.

Now I'm doing a kind of percolation: 
\begin{itemize}
\item
keep $v\in D_k$ if $\fc{S_v}k \ge y$
\item
delete $v$ otherwise
\item
go at level $2k$, etc.
\end{itemize}•
Continue only vertices which I kept at the first step, where the mean is large enough.
\end{enumerate}•
Now I have an embedded GW-tree $\wt T\sub T$. If $\wt T$ is infinite, the maximum is at least $y$.
% With positive probability, the maximum is at least $y$.

Question: what is $\wt m$? We have
\begin{align*}
\wt m &= m^k \Pj[S_k \ge ky]\\
&\ge e^{k\log m} e^{-k(I(y)-\ep)} \ge e^{\ep k}>1
\end{align*}•
for $k$ large enough.
Then 
\begin{align*}
\Pj[\lim_{n\to \iy} \inf \fc{M_n}{n} \ge y] &>0.
\end{align*}•

Now we need a 0-1-law for inherited properties. Call a property $A$ of trees \vocab{inherited} if each finite trees has $A$, and if $T$ has $A$, then all descendant trees of the children of the root have it. 
Then $\Pj^*[T\text{ has }A]\in \{0,1\}$.
Thus we conclude the probability is 1.
\end{proof}
\begin{proof}[Proof of 0-1 law]
We have
\begin{align*}
\Pj[T\text{ has }A] &= \E[\Pj[T\text{ has }A||D_1|]]\\
&\le \E[\Pj[T^{(1)}\text{ has }A,\ldots, T^{(|D_1|)}\text{ has }A]||D_1|]
&\text{inherited property}\\
&= \E[\Pj[T\text{ has }A]^{|D_1|}]&\text{independence}
\end{align*}•
Hence 
\begin{align*}
\ga :=\Pj[T\text{ has }A] &\le f(\Pj[T\text{ has }A])
\end{align*}•
where $f(s) =\sumz k\iy s^kp(k)= \E[s^{|D_1|}]$. 
So $q\le \ga \le f(\ga)$.
On the other  hand
%extinction probability
\begin{align*}
\Pj[T\text{ has }A] &\ge 0\\
q:&= \Pj[\lim_{n} |D_n|=0]
\end{align*}•
This means that 
\begin{align*}
\Pj[T\text{ has }A] &\in \{q,1\}.
\end{align*}•
(The probability is in $[q,1]$ because it is at least as big as the probability of finiteness. The function $f(s)$ is convex---its second derivative is positive---so its graph is below the line connecting the two fixed points.)
%orig measure p
But if we condition, we get
\begin{align*}
\Pj^*[T\text{ has }A]&\in \{0,1\}
\end{align*}•
%The event $\{\liminf M_n<nY\}$ (complement of considered event) is inherited.
%Look at the following property $A$:
%$$A=\{T\text{ finite}\}\cup \{\liminf \fc{M_n}n\le y\}.$$
\end{proof}

\subsection*{2020/6/2 Lecture 2}

\begin{rem}
If $\Pj[X\ge t] = e^{-ct^r}$ where $0<r<1$ (stretched exponential tails) and $\E[X^2]<\iy$ (finite second moment) then $$\rc{n}\log \Pj[S_n\ge n^{1/r} y]\to -cy^n\text{ as }n\to \iy.$$
%deviation controlled by max
The same arguments give that 
\begin{align*}
\fc{M_n}{n^{1/r}} \to x^* = \pf{\log m}{c}^{1/r}.
\end{align*}•
Note the position of the rightmost particle grows faster than linearly. $x^*$ is such that the two effects balance: the number of particles growing exponentially, and the probability decreasing exponentially (with a power).
See the talk ``Stretched exponential tails," on Friday, by Piotr Dyszewski.
\end{rem}

\begin{rem}
Let $X,X_1,X_2,\ldots $ be iid, $S_n=\sumo in X_i$. Let $S_n, S_n^{(1)},S_n^{(2)},\ldots$ be iid. Let $\wt M_n = \max_{1\le i\le |D_n|} S_n^{(i)}$. We showed by a union bound that %$m^n$ particles 
\begin{align*}
\Pj[\wt M_n \ge ny] &\le m^n \Pj[S_n\ge ny]\\
\limsup \fc{\wt M_n}{n} &\le x^*.
\end{align*}•
\end{rem}
We can compare $M_n$ (from the BRW) and $\wt M_n$: The maximum of the branching random walk is smaller than the maximum of the independent random walks.
\begin{lem}\label{l:brw1}
$M_n\le \wt M_n$. (Here $\le$ means stochastically dominated.)
\end{lem}
The result is not surprising because there are more independent random variables involved in $\wt M_n$.
\begin{lem}\label{l:brw2}
Let $(X_i)_{i\ge 1}$, $(Y_i)_{i\ge 1}$ be independent, and $(Y_i)_{i\ge 1}$ identically distributed. Then %we can compare the maxima in the following sense
\begin{align*}
\max_{1\le i\le n}(X_i + Y_1) &\le \max_{1\le i\le n}(X_i+Y_i).
\end{align*}•
\end{lem}
\begin{exr}
Prove Lemma~\ref{l:brw2}.
\end{exr}
\begin{proof}[Proof of Lemma~\ref{l:brw1}]
Proof is by induction. 
Apply the induction hypothesis to the subtrees of the root. By lemma 2, we can replace the identical variables in the edges coming from the root with iid variables, and get something larger.
\end{proof}
\begin{exr}
\begin{enumerate}
\item
Show $\fc{\wt M_n}{n}\to x^*$. %One direction was already proved.
\item
Suppose $\Pj[X=1] = \al = 1-\Pj[X=-1]$. 
%lattice-valued random walk
Will the cloud visit 0 infinitely often? I.e., 
\begin{align*}
\ga &= \Pj[S_v = 0 \text{ for some }v\in D_u, \text{ for infinitely many }n].
\end{align*}•
%For 1 particle: when \al=1/2
%(If $\al$ gets close to 1 this gets less likely.)
Find conditions on $\al$ and $m$ such that $\ga=0$ or such that $\ga>0$.
%condition will involve branching.
%1st moment for ub, embedded tree for lb.
%not claiming necessary and sufficient.
\item
Do the same for a tree-indexed (branching) Markov chain. We have a Galton-Watson tree with a Markov chain: 
\begin{enumerate}
\item
particles reproduce according to $p(\cdot)$
\item
offspring takes a step according to %the Markov chain 
$q(x,\cdot)$, where $q(x,y)_{x,y\in S}$ are transition probabilities of an irreducible Markov chain, with $S$ countable.
\end{enumerate}•
Take $0\in S$, define $\ga$ as before. 
Find conditions on $m$, %mean of branching
$q(\cdot, \cdot)$ such that $\ga=0$ or $\ga >0$.

This is a generalization of the previous example.
\end{enumerate}•
\end{exr}
To solve the recurrence-transience questions, re-run the argument we saw yesterday. I don't claim they are necessary and sufficient in all cases, but in almost all cases. (The arguments do not determine what happens at the critical point.)

What is the second order term, i.e., $M_n - nx^*\sim $? E. A\"id\'ekou proved the following.
\begin{thm}
Suppose 
\begin{itemize}
\item
$X$ is non-lattice (this is important),
\item 
$X$ has finite exponential moments,
$\E[e^{\la X}]<\iy$ for all $\la\in \R$. 
\item 
$\log m \in \text{int}\set{y}{I(y)<\iy}$ (int means interior).
\item
$\E[|D_1|^{1+\de}]<\iy$ for some $\de>0$.
\end{itemize}•
 Then
\begin{align*}
\Pj[M_n- x^*n + \fc{3}{2\ol \la }\log n\le t]
&\xra{n\to \iy} \E[\exp(-CZ_\iy e^{-t})]
\end{align*}•
where $\ol \la = I'(x^*)$ and $Z_\iy$ is the a.s. limit of $Z_n = -\sum_{v\in D_n} (S_v-nx^*) e^{\ol \la (S_v - nx^*)}$.
%one has to prove this converges
\end{thm}
This is the Gumbel distribution with a random shift.
\begin{exr}
Check that $(Z_n)$ is a martingale with respect to $(\cal F_n)$, $\cF_n=\si\pat{``up to level n"}$, the ``derivative martingale."
\end{exr}
The limit of the martingale is strictly positive. 
Showing it converges is more difficult; the result goes back to Biggins.

The theorem was first proved for Brownian motion, and took 20 years before it was shown for branching random walks.
%so shi
%plausibility argument

An easier theorem is the following. %for reference model. 
\begin{thm}
Assume the same assumptions, $p(m)=1$. Then
\begin{align*}
\Pj[\wt M_n - x^* n + \rc{2\ol \la }\log n\le t]
\xra{n\to \iy} \exp(-Ce^{-\ol \la t}). 
\end{align*}•
\end{thm}
The reference model also gives a Gumbel distribution. Note the second term has a different constant. I'll give a proof sketch; it can easily be made precise in the gaussian case. 
\begin{proof}[Proof sketch]
Note that for $a_n=o(\sqrt n)$, using the non-lattice assumption, by the Bahaduv-Rao Theorem,
\begin{align*}
\Pj[S_n > nx^* - a_n] &\approx \fc{C}{\sqrt n} e^{-nI(x^*-\fc{a_n}{n})}.
\end{align*}•
(Here $\approx$ means if you divide you will have bounds by constants.)
But 
\begin{align*}
nI(x^*-\fc{a_n}n) &= n\ub{I(x^*)}{\log m} - \ub{I'(x^*)}{\ol \la}a_n + o(1).
\end{align*}
Plugging in and noting $\wt M_n$ is a maximum of independent $S_n$'s gives
\begin{align*}
\Pj[\wt M_n \le nx^* - a_n]
&\sim \pa{1-\fc{C}{m^n\sqrt n} e^{\ol \la a_n + o(1)}}^{m^n}.
\end{align*}•
Choose $a_n = \fc{\log n}{2\ol \la} - t$ to get this probability converges:
\begin{align*}
\Pj[\wt M_n \le nx^* - a_n]
&\sim \exp(-Ce^{-\ol \la t}+o(1)).
\end{align*}•
\end{proof}
\begin{exr}
\begin{enumerate}
\item
Make this argument precise for $X\stackrel d= N(0,1)$. 
\item
Prove that $\E\ba{\fc{\wt M_n}n}\to x^*$ and conclude
\begin{align*}
\limsup_{n\to \iy} \E\ba{\fc{M_n}n} &\le x^*.
\end{align*}•
\end{enumerate}•
\end{exr}
There are plenty of open questions for $d\ge 2$. The field starts with BBM: J Berestycki, B. Mallein, J. Schweinsberg...

\subsection*{2020/6/4 Lecture 3}

Today there will be less proofs and more open questions.

Consider an example of a BRW with killing: $p(2)=1$, $\Pj[X=0] = 1-p$ and $\Pj[X=1]=p$. 
By the exercise in Lecture 1, we know how the maximum will behave. I'll ask a different question now. 

A ray is $v_0,v_1,v_2,v_3,\ldots$ where $v_0$ is the root and $v_{i+1}$ is a child of $v_i$. Is there a nearly optimal ray? Define
\begin{align*}
\rh(\ep, p) &= \Pj[\exists \text{ infinitely ray such that }S_{v_j} \ge (x^*-\ep) j, \forall j\ge 1]\\
&= \Pj[\text{``nearly optimal ray"}]
\end{align*}•
This comes up in computer science in search trees. I'm ``killing" all particles below the line with slope $x^*-\ep$.
%infimum of means

Question: $\rh(\ep,\rh)\sim $? for fixed $p$, as $\ep\to 0$. %simple cases
\begin{itemize}
\item
For $p>\rc 2$, $x^*=1$, with positive probability there is an optimal ray, so $\rh(\ep, \rh)\not\to0$ as $\ep\to 0$.
\item
For $p=\rc2$, this is much less clear. We don't have an infinite cluster if we keep edges with 1. We have $\rh(\ep, p)\sim c\ep$.

This goes back to Robin Pemantle; he also formulated a conjecture for $p<\rc 2$. 

Take $\ep=\rc n$ ($n=\rc{\ep}$). To survive until time $n$, you need to have 1's up to time $n$. The probability is bounded by 
\begin{align*}
\Pj_{\rc 2}[\exists \text{ ray with only 1's up to level }n]&\sim \fc2n.
\end{align*}•
This is the probability a critical GW process suvives up to generation $n$, which is known to be $\sim \fc 2n$.
% In a critical branching process, the probability to survive up to level $n$ is $\sim \fc 2n$.

The argument gives an upper bound: If you survive infinitely you have to survive to level $n$. You can give a lower bound with a more involved branching process argument.
\item
For $p<\rc n$, 
\begin{align*}
\log \rh(\ep, p)&\sim \fc{-c(p)}{\sqrt \ep} \text{ for }\ep\to 0.
\end{align*}•
This is a result by NG, Y. Hu, and Z. Shi. %We proved it by looking at the bran %Yueyun
The idea is similar but the implementation is more involved.
%implementation is 

Replace the large deviation estimate for \emph{endpoint} of path, which has the same law as sum of iid random variable, with large deviations for the \emph{whole path}.
%at heart of survival prob asymptotics

%transform linearly above certain line

\begin{thm}[Mogulskii]
Let $Y_1,Y_2,\ldots$ be iid with $\E[Y_i]=0$, $\E[|Y_1|^{2+\de}]<\iy$, $S_n = \sumo in Y_i$, $g_1(\cdot)$, $g_2(\cdot)$ continuous, $g_1(0)<0<g_2(0)$. Then
\begin{align*}
\fc{a_n^2}{n}
\log \Pj[g_1\pf in \le \fc{S_i}{a_n} \le g_2\pf in, 1\le i\le n]
&\to 
\exists (g_1,g_2,\si^2)
\end{align*}•
Here $a_n\to \iy$, but $\fc{a_n}{\sqrt n}\to 0$.
\end{thm}
 This decays slower than exponential. 
This is at the heart of the survival probability.
%above a certain slope
\end{itemize}•

I'll mention another model with selection, the $N$-BRW.
\begin{itemize}
\item
Keep only the $N$ individuals with the largest positions.
\end{itemize}•
This was introduces by Buouet, Derrida, Mueller, and Meunier. Let $M_{n,N}=\max_{v\in D_{n,N}}S_v$. Then
\begin{align*}
\fc{M_{n,N}}{n}\xra{n\to \iy} x_N^*\text{ a.s.}
\end{align*}•
(Has the largest one has always been the largest, or do other particles pass it?)
It is clear that $x_N^*\le x^*$; the question is how does the difference behave?
\begin{thm}[J. B\'erard, J.-B. Gouer\'e]
For $p(2)=1$, $\E[e^{\la X}]<\iy$ for all $\la \in \R$. Then
\begin{align*}
x^* - x_N^* \sim \fc{c}{(\log N)^2}\text{ for }N\to \iy.
\end{align*}•
\end{thm}
J. B\'erard and P. Maillard showed for heavy-tailed $X$ that
\begin{align*}
\Pj[X>t] &\sim \rc{t^\al}, \quad 0<\al<2.
\end{align*}•
%heuristic reason why related to random walk with killing.
%P(X>x)\sim c, x^{-\alpha}, for any \alpha>0. conv of the stairs process

I'll give a heuristic argument of how this is related to the survival probability of BRW with killing.

The following two events are comparable:
\begin{enumerate}
\item
BRW with killing at slope $x^*-\ep$, starting with $N$ particles, survives.
\item 
$N$-BRW moves at a speed $\ge x^*-\ep$.
\end{enumerate}•
In  
(1) the probability is $1-(1-\rh(\ep))^N$. 

So in 
(2), $x^*-x_N^*$ should be of order $\ep=\ep(N)$ with $\ep$ such that $\rh(\ep)\approx \rc{N}$.
%law of \epsilon

But since $\rh(\ep)\approx e^{-\fc{c}{\sqrt \ep}}\approx \rc N$, then we have $\ep\approx \fc{c}{(\log N)^2}$. The proof does not use this heuristic, but confirms it.
The proof is relatively technical; it would be nice to have a simple proof.

I'll give another example with selection, the L-BRW.
\begin{itemize}
\item
Keep only the individuals within (spatial) distance $L$ to the maximum $M_n$, remove all the others.
\end{itemize}•
Define
\begin{align*}
x_L^* &= \lim_{n\to \iy}\fc{M_{n,L}}n.
\end{align*}•
It is clear that $x_L^*\le x_L$. %The conjecture (BDMM) is that 
\begin{conj}[BDMM]
$x_L^*\approx x^*$ %_L
with $L=\log N$.
\end{conj} 
%$x^*-x_L^*=\fc{c}{L^2}$.
This is open!
This theorem has been confirmed for L-BBM by M. Pain (recent).
%branching brownian motion

BRW with interaction: As soon as you introduce interactions, the methods are less clear.
%understand in epidemiological context

Q: Take a BRW with some $X\stackrel d= N(0,1)$.  Fix $R\in \R$.
\begin{itemize}
\item
kill two particles if they come too close: closer to $R$.
\end{itemize}•
Is the survival probability strictly positive?
Even in 1-D the problem is completely open.

\paragraph{Fragmentation process.} $(I_t)_{t\ge 0}$ is a collection of disjoint intervals $\subeq (0,1)$. An interval $(a,b)$ with $u=b-a$ splits  at rate $u^\al$ into $m$ subintervals $(a, a+\fc{u}{m},a+\fc{2u}m,\ldots, b)$. 
Let $N_t(j)$ be the number of intervals $(a,b)$ at time time $t$ such that $b-a = m^{-j}$.

We can see this as a multitype branching process.
\begin{itemize}
\item
Multitype branching process: Types in $\N_0$. Particles die at a rate $q^j$ which depend on time, replaced by $m$ offspring of type $j+1$.

Let $N_t(j)$ be the number of particles of type $j$ present at time $t$.
\item
The tree-indexed RW is not homogeneous anymore. On edges we have independence variables $(W_e)\stackrel d= \text{Exp}(q^n)$ if $e$ goes from level $n-1$ to level $n$.
At time $t$, we have a certain number of particles which are present for some lifetime.

Say $v$ is alive at time $t$ if $S_v\le t$, $S_{v_i}>t$ for all $1\le i\le m$ for all children $v_i$ of $v$.

We can study $N_t=\sum_{n \ge 0} N_t(u)$.
%Minimal type
%smallest interval at time t.
\item
Work in progress with P. Dyszewski, S. Johnston, J. Puochno, and D. Schmid.
\end{itemize}•
\begin{thm}[Brennan, Durrett]
$\E[N_t]\sim t^\be $ where $\be = \fc{\log m}{\log \rc q}$, $q=m^{-\al}$. 
\end{thm}
%$q$ related to $\al$.
%number of vertices present at time t, sum over levels.
%bound on rv's not iid but independent.
%We believe we can refine the result.
%limit laws
%particle in gen g as corresponding to interval of length m^{-g}. how to translate q and alpha

%heuristics on properties regarding BRW with local interactions.
%I don't have conjecture for quantitative.

%\subsection{}
%%Kristen-Stigum condition.
%\begin{thm}
%Take $X$ such that $\E X$ and $\E X^2=1$. 
%Suppose that $X$ has a streteched exponential tail, i.e., $\Pj(X>t)\sim e^{-t^r}$. 
%Then there are constants $\ga, \si, \al$ such that:
%\begin{itemize}
%\item
%for $r\in (0,\fc 23]$, 
%\begin{align*}
%\fc{M_n-\al n^{1/r}}{\si n^{1/r-1}}
%\end{align*}•
%\end{itemize}•
%%Either fluctuates or has 
%\end{thm}
%where exponent, behavior coming from. How to use heavy tails to your advantage?
%
%Let $(Z_n)$ be a Galton-Watson process with reproduction mean $m>1$. Assume for simplicity $\Pj[Z_1=0]=0$. Let $\bT\subeq \bigcup_{n\ge 0}\N^n$ be the corresponding Galton-Watson tree.
%
%Take $X_y$, $y\in \bT$ iid. For a vertex $x\in \bT$, $V(x) = \sum_{y\le x} X_y$. 
%
%The lack of exponential moments translates to the principle of ``one big jump." 
%$\Pj[X>t]\sim e^{-t^r}$, $r\in (0,1)$ implies
%\begin{align*}
%\Pj[X_1+X_2>t] &\sim \Pj[\max\{X_1,X_2\}>t].
%\end{align*}•
%We try to get a large deviation result out of this phenomena. For $S_n = \sum_{1\le k\le n}X_k$, $X_n^*=\max_{1\le k\le n}X_k$, $\Pj[S_n>t_n]\sim$?
%\begin{align*}
%\Pj[S_n > cn^{1/r}] &\approx
%\sup_s \Pj\ba{X_n^*> cn^{\rc r}-s}\Pj[S_{n-1}>s]\\
%&\approx \sup_s n \exp\ba{-\pa{cn^{\rc r}-s}^r - \fc{s^2}{2n}}\\
%&\approx \Pj\ba{X_n^* > cn^{\rc 3} - rc^{r-1}n^{2-\rc r}}\Pj\ba{S_{n-1}>rc^{r-1}n^{2-\rc r}}.
%\end{align*}•
%%Optimize.
%The inequality can be rewritten $\fc{S_{n-1}}{\sqrt n}>rc^{r-1}n^{\fc 32-\rc r}$.
%So $r=\fc 23$ is critical for behavior of deviations.
%\begin{align*}
%\Pj\ba{S_n> cn^{\rc r}} &\sim n\exp\ba{-c^r n + O(n^{3-\fc 23})}.
%\end{align*}•
%If $|x|=n$, then
%\begin{align*}
%V(x) \stackrel d S_n = \sumo kn X_k.
%\end{align*}•
%We get
%\begin{align*}
%\Pj\ba{M_n> cn^{\rc r}}&\le \E[Z_n] \Pj\ba{S_n>cn^{\rc r}}\\
%&\sim m^n \exp\ba{-c^r n (1+o(1))}.
%\end{align*}•
%We want $-c^rn$ to balance out $m^n$. $\fc{M_n}{n^{\rc r}}\to \al =\log(m)^{\rc r}$.
%
%The first term in the asymptotic expansion of $M_n$ is related to the biggest displacement, i.e., $N_n=\max_{|y|\le n}X_y$.
%After proper scaling, we get the Gumbel distribution with a random shift.
%\begin{pr}
%$$\fc{N_n-\al n^{\rc r}}{\si n^{\rc r-1}} \to^d H\stackrel d=\E[\exp\ba{-\ga' We^{-x}}].$$
%\end{pr}
%\begin{proof}
%\begin{align*}
%\Pj[N_n\le x_n] &\sim \E[\exp[-Y_n \Pj[X>x_n]]].
%\end{align*}•
%where $Y_n$...
%
%$m^{-n}Z_n$ is a positive martingale.
%
%%Assuming the KS condition, this will be nonnegative.
%Take a concrete threshold $x_n = \al n^{\rc n}+\si n^{\rc r-1}x$ and compute $Y_n \Pj[X>x_n]$. 
%Exponent $\rc r-1$ tailored so that it will cancel with derivative that comes up.
%%Taylor expansion
%$\al$ is taken so that $-\al^r n$ cancels with $m^n$.
%\end{proof}
%Concentrate on particles that had a big jump.
%
%When $r$ is big, see something that is almost exponential. Small $r$, small number of big jumps that contribute.
%\begin{align*}
%\#\set{|v|\le n}{X_v\approx \al n^{\rc r}}&\approx m^{o(n)}=\begin{cases}
%...&...
%\end{cases}•
%\end{align*}•
%
%Color the edges green, color the edges below red. Look at paths from the greedn edges to the root. Orange: disjoint from other paths. Paint blue when meet each other.
%\begin{align*}
%V(x) &= \blu{R_1(x)}+{\color{orange}V_0(x)} + {\color{green}N(x)}+\redd{R_2(x)}.
%\end{align*}•
%Paths meet high up, blue bit is short, negligible.
%
%Maximum displacement grows faster than linearly. More likely happen in generation close to $n$. Red segments are short.
%
%Typical particles... Behavior should be asymptotically normal. Do the same optimization game. Contribution 
%\begin{align*}
%V_0(x) &= O(n^{2-\rc r}).
%\end{align*}•
%If $r<\fc 23$, $n^{2-\rc r} = o(n^{\rc r-1})$. %small compared to biggest
%$$\fc{M_n-\al n^{\rc r}}{\si n^{\rc r-1}}\sim \fc{N_n-\al n^{\rc r}}{\si n^{\rc r-1}}\to^d H.$$
%We can show $\fc{M_n-N_n}{n^{\rc r-1}}\to 0$. (rightmost particles vs. biggest fluctuation)
%
%$M_n \approx \max_x \{V_0(x) + N(x)\}$.
%
%What happens in the boundary case? If $r=\fc 23$, $n^{2-\rc r} = n^{\rc r-1}=\sqrt n$, so fluctuate.
%$\cal N_x \stackrel d \Phi(\cdot)\stackrel d= \cal N(0,1)$. 
%\begin{align*}
%\fc{M_n-\al n^{\fc 32}}{\si\sqrt n} &\approx \max_x \bc{\fc{N(x)-\al n^{\fc 32}}{\si\sqrt n}+\rc \si \cal N_x}.
%\end{align*}•
%Decouple whole thing, so work with independent variables. 
%$\fc{N(x)-...}{...}$ term transfer to $e^{-y}$, Gaussian rv transfers to $\Phi$ term. Still $c\cdot e^{-t}$.
%
%One big jump supplemented. Contribution hidden in constant $\ga$. Not continuous function of our parameters.
%
%\arxiv{2004.03871}.
%
%\subsection{The extremal particles of branching Brownian motion}
%
%\begin{itemize}
%\item
%Start with single particle at origin.
%\item
%Move like Brownian motion.
%\item
%At rate 1, particles split into 2.
%\end{itemize}•
%We are interested in the extremal particles: the few particles with highest spatial positions at large times $T$.
%
%We'll focus on 3 questions.
%\begin{enumerate}
%\item
%Where are they?
%\item
%How did they get there? What is the trajectory of the particle with the highest spatial position at a large time $T$?
%\item 
%Are they closely related?
%Take the two highest particles at a large time $T$. When did they most recently share a common ancestor?
%\end{enumerate}•
%%import results from BRW
%$T$ is a huge time.
%
%Where are they? 
%
%Let $N_T$ be the number of particles alive at time $T$. Then $\E[N_T]=e^T$. If you consider the ratio $Z_t=\fc{N_t}{e^t}$, then $Z_t$ converges to a (random) limit. The height $Z_\iy$ is exponentially distributed with rate 1.
%
%According to the many-to-one formula, the expected number of particles above $y$ at time $T$ is $\E[N_T^{\ge y}] = \E[N_T] \Pj(B_T\ge y)$, where $B_T$ is position of Brownian motion at time $T$.
%\begin{align*}
%\E[N_T^{\ge y}] &= e^T \int_{y/\sqrt T}^\iy \rc{\sqrt{2\pi}}e^{-u^2/2}\,du.
%\end{align*}•
%Choose $y=y_T$ so that this quantity has order 1.
%\begin{align*}
%y&=\sqrt 2 T - \rc{2\sqrt 2}\log T + O(1).
%\end{align*}•
%Expected number of particles above is tiny, and below is huge. It is natural to guess that the highest particle in the process at time $T$ is $\sqrt 2 T - \rc{2\sqrt 2}\log T$. However, this is wrong! 
%\begin{thm}[Bramson] 
%The highest particle sits at $\sqrt 2 T - \fc{3}{2\sqrt 2}\log T+O(1)$ whp.
%\end{thm}
%On average there are loads of particles between expectation and reality, but usually there are no particles there.
%
%Hu and Shi: 
%\begin{align*}
%\limsup_t \fc{\max_T -\sqrt{2}t}{\log t}&=-\rc{2\sqrt 2}\\
%\liminf_t \fc{\max_T -\sqrt{2}t}{\log t}&=-\fc3{2\sqrt 2}
%\end{align*}•
%``Hidden events" happen occasionally.
%
%How did they get there? 
%We know the highest particle is near $\sqrt 2T - \fc{3}{2\sqrt 2}\log T$. A natural first guess:  The trajectory of the highest particle looks like Brownian motion conditioned on $B_T=\sqrt 2T - \fc{3}{2\sqrt 2}T$.  However, this process would occasionally go $O(\sqrt T)$ above $\sqrt 2t$. According to Hu and Hi, no one ever goes above $\sqrt t - \rc{2\sqrt 2}\log t$.
%
%A better guess: it shouldn't ever go above the line $\sqrt t - \rc{2\sqrt 2}\log t$.
%
%Theorem: Chen (2015) The past trajectory of the highest particle looks like Brownian motion conditioned to end up at $\sqrt 2T - \fc{3}{2\sqrt 2}\log T$, but conditioned to never go above the line $\sqrt t - \rc{2\sqrt 2}\log t$.
%
%Are they closely related?
%
%If we take 2 typical particles, it's not obvious when they were last related. 
%Harris, J, and Roberts (2017)
%Pick 2 particles uniformly at time $T$. Let $\Pi_T^{\text{UNI}}$ be the time of the common ancestor. Then
%\begin{align*}
%\lim_{T\to \iy} \Pj(\pi_T^{\text{UNI}}>s) &= \fc{2e^{-s}}{(1-e^{-s})^2}...
%\end{align*}•
%Pick particles according to Gibbs measure
%\begin{align*}
%\Pj^\be \pa{\text{Pick }(u,v)} &= \fc{e^{\be x_T^u}e^{\be x_T^v}}{...}
%\end{align*}•
%\begin{thm}[Derrida... Bovier]
%Pick 2 different particles according to $\be$-Gibbs measure. If $\be>\sqrt 2$, then $\pi_T$ is either $O(1)$ or $T-O(1)$ with high probability (and both with positive probability). 
%\end{thm}
%Let $p_\be = \Pj^{\be\text{-Gibbs}}\pat{related early}$. Chen, Maduale, and Mallein calculated $p_\be$ explicitly. Poisson-Dirichlet distribution. $p_\be$ is decreasing in $\be$. $p_\be=1$ when $\be<1$.
%Related work: Arguin, Bovier, Kistler (2011), Maduale (2017), Pain (2018).
%
%Recap:
%\begin{enumerate}
%\item
%The highest particles are usually near $\sqrt 2T - \fc{3}{2\sqrt 2}\log T$ (but sometimes a bit higher!).
%\item
%Their trajectories look like Brownian bridges with repulsion.
%\item
%They are related either right at the start or right at the end.
%\end{enumerate}•
%%$\be\to \iy after T\to \iy. When $\be>2$ or $\sqrt 2$, extremely likely you pick the top 2.