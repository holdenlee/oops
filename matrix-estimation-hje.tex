
~

These are notes from the Online Open Probability School (OOPS) 2020. 
\begin{itemize}
\item
Webpage for OOPS: \url{https://www.math.ubc.ca/Links/OOPS/}.
\item
Source is available at \url{https://github.com/holdenlee/oops}. Contributions, corrections, and comments welcome; feel free to send a pull request. Any errors are probably due to me! You can ping me on zulip or email me at \texttt{holden.lee@duke.edu}.

A direct link to these notes is \url{https://www.dropbox.com/s/u4lpayi98r06un5/oops.pdf?dl=0}.
\end{itemize}

\section{Disordered systems, rank-one matrix estimation and Hamilton-Jacobi equations (Jean-Christophe Mourrat)}

We consider the problem of estimating a large rank-one matrix, given noisy observations. This inference problem is known to have a phase transition, in the sense that the partial recovery of the original matrix is only possible if the signal-to-noise ratio exceeds a (non-zero) value. We will present a new proof of this fact based on the study of a Hamilton-Jacobi equation. This alternative argument allows to obtain better rates of convergence, and also seems more amenable to extensions to other models such as spin glasses. 

References:
\begin{itemize}
\item
Paper: \cite{mourrat2018hamilton}, \url{https://cims.nyu.edu/~jcm777/HJinfer.pdf} %, \url{https://arxiv.org/abs/1811.01432}.

Approach based on weak solutions: \cite{mourrat2019hamilton}, \url{https://cims.nyu.edu/~jcm777/HJrank.pdf} %, \url{https://arxiv.org/abs/1904.05294}


\item
Book for background material: \cite{friedli2017statistical}, available at \url{https://www.unige.ch/math/folks/velenik/smbook/index.html}
\item Other approaches
\begin{itemize}
\item
Lelarge-Miolane: \cite{lelarge2019fundamental}, \url{https://arxiv.org/abs/1611.03888}
\item 
Barbier-Macris: \cite{barbier2019adaptive}, \url{https://arxiv.org/abs/1705.02780}
\end{itemize}•
\end{itemize}• 
%Lelarge and Miolane have a paper on this rank-one estimation problem (probably the first one with a complete solution, and beautifully written), but the people who construct the characteristics for finite N are Barbier-Macris.

\subsection*{2020/5/18 Lecture 1}

Suppose students are assigned one of two dormitories. They put on a sorting hat, which decides which dorm they go in.

The students are $i\in \{1,\ldots, N\}$. An assignment is $\si\in \{\pm 1\}^N$. 
The sorting had optimizes the quality of interaction between $i$ and $j$, $J_{ij}$. Suppose $(J_{ij})$ are independent standard Gaussians. The larger $J_{ij}$ is, the more that $i$ and $j$ like to be together. We want to maximize $\si\mapsto \sum J_{ij} \one_{\{\si_i=\si_j\}}$. By a linear transformation this is equivalent to maximizing $\sum J_{ij} \si_i\si_j$.

What is $\max_{\si\{\pm 1\}^N} \sum J_{ij}\si_i\si_j$ as $N\to \iy$?
Because the $J_{ij}$ can be positive or negative, we can't make all the students happy. Thus we can say there are \vocab{frustrations} in the problem. These models are called spin glasses in the literature.
It's difficult to find the optimum: making local moves, you may have to decrease the objective before increasing it.

I want to consider a softer version of the maximum. We look at the \vocab{spin glass model}\footnote{Note that the normalization is $\rc N$ instead of $\rc{\sqrt N}$ because here the $J_{ij}=1$.}
\begin{align*}
\E \rc N\log \sum_{\si\in \{\pm 1\}^N}\exp\pa{
\fc{\be}{\sqrt N}\sumo{i,j}N J_{ij} \si_i\si_j
}
\end{align*}•
If $\be$ is large this is dominated by the maximum. 
This is like a relaxation of the problem. We should expect what's inside is order $N$, so we divide by $N$.\footnote{Can we fix the number of $+1$'s and $-1$'s? You can change the reference measure; this can be encoded as changing the reference measure.
$\be=0$ is summing over reference measure. $\be\to \iy$ recovers the maximum. $\be$ small is high temperature, $\be$ large is small temperature.}

Parisi in the late 70's (1979) proposed an answer for what this becomes as $N\to \iy$. It's a fairly complicated formula. 

%disordered high-order system, hard to think of something simpler. 
Guerra 03 and Talagrand 06 proved it rigorously. I find it mysterious; I want to think about a slight variation of the problem. Instead of connections between each $i,j$, think of them organized in two layers; there are interactions between but not within the layers (the graph is bipartite). 
%hard to understand 
This seems an innocent modification, but I could not understand what to write instead of the complicated formula! %good sign to think more.

%This is called the spin-glass model.

Now I consider rank-one matrix esimation/inference. The question is statistical: we only observe a noisy version of a rank-one matrix. Can we recover information about it?

%Consider it a large matrix. 
Here are some concrete settings where this is useful:
%A concrete setting: 
\begin{itemize}
\item
You are Netflix, you want to make recommendations for your customers. A simple model is that whether or not a person likes a movie is captured by a few parameters of the movie (action, introspection, sad/happy, etc.) and customer, and is a linear function of the parameters. Then you have a large low-rank matrix. A simplification is that it's a matrix of rank 1. I'll describe rank 1, but it's not hard to generalize. 
\item
%Another example is 
Community detection: The US is polarized, and there is a binary variable that will predict whether two people will be friends.
\end{itemize}•

The common thread is the relation with certain partial differential equations, called \vocab{Hamilton-Jacobi equations}.

The \vocab{Curie-Weiss model} is a simple model that can be solved in many ways. I want to emphasize the method that uses intuition with Hamilton-Jacobi equations. Next when we turn to rank-1 matrix estimation, the proof will be almost the same. 

Our derivation is not standard; if you want to see a more standard derivation see Friedli and Velenik~\cite{friedli2017statistical}.

\emph{Can you meaningfully recover information about the rank-1 matrix?} In the Ising model there is a phase transition between an ordered and disordered state. In this inference problem there is also a phase transition. When signal-to-noise ratio is too small (weak), you cannot recover meaningful information. After the threshold, you can recover partial information. 



\subsection{Definitions}

We want to study the probability measure that to each $\si\in \{\pm1\}^N$, associates a weight proportional to 
\begin{align*}
\exp\pa{\fc{t}{N} \sumo{i,j}N \si_i\si_j + h\sumo iN \si_i}.
\end{align*}•
The second term doesn't have interaction; it ``tilts" the $\si_i$, giving each a preference. Here $t>0$ but $h\in \R$.
Define the expected value
\begin{align*}
\an{f(\si)}_{k,h} :&= \fc{\sum_\si f(\si) \exp\pa{\fc{t}{N} \sumo{i,j}N \si_i\si_j + h\sumo iN \si_i}}{\sum_\si \exp\pa{\fc{t}{N} \sumo{i,j}N \si_i\si_j + h\sumo iN \si_i}}.
\end{align*}•
The subscripts are omitted when clear.

Define the free energy
\begin{align*}
F_N(t,h) &=\rc N \log \sum_\si \exp\pa{\fc{t}{N} \sumo{i,j}N \si_i\si_j + h\sumo iN \si_i}
\end{align*}•
You might say: this is the normalization constant, we care about the measure. This is misleading because the normalization constant is the generating function of quantities we care about. You are calculating the exponential (moment) generating function of these variables. If you understand the mgf, you understand these quantities.

%inside order N.
%extensive quantity 
%in first 1/\sqrt N,  because centered.
\paragraph{Moment generating function.}
Differentiating gives
\begin{align}
\label{e:pd-h}
\pl_h F_N &= \rc N \fc{\sum_\si \pa{\sumo iN \si_i} \exp\pa{\fc{t}{N} \sumo{i,j}N \si_i\si_j + h\sumo iN \si_i}}{\sum_\si \exp\pa{\fc{t}{N} \sumo{i,j}N \si_i\si_j + h\sumo iN \si_i}} = \rc N \an{\sum_i \si_i}\\
\pl_t F_N &= \rc N \an{\rc N\sum \si_i\si_j} 
= \an{\pa{\rc N \sum \si_i}^2}.
\end{align}•
This model is simple; I can rewrite $\pl_t F_N$ in a simple way. The derivatives are all order 1.
It's a good starting point to notice that
\begin{align*}
\pl_t  F_N - (\pl_hF_N)^2 &= 
\an{\pa{\rc N \sum \si_i}^2} - \an{\rc N \sum \si_i}^2.
\end{align*}•
This is the mean magnetization, the variance of the magnetization.
Idea: The variance is lower-order, so as $N\to \iy$, $F_N$ solves the equation with 0 on the right.

$F_N$ is the mgf of $\sum\si_i$. So in particular it should encode the variance of the variable in some way. I should find a way to express it in terms of $F_N$. Looking at the second derivative is a good idea.
\begin{align*}
\pl_h^2 F_N &= \rc N \an{\pa{\sum \si_i}^2} - \rc N\pa{ \an{\sum \si_i}}^2.
\end{align*}•
So we have shown
\begin{align}
\pl_t F_N - (\pl_h F_N)^2 &= \rc N \pl_h^2 F_N.
\label{e:pde1}
\end{align}•
This is a very important observation: everything is expressed in terms of $F_N$. We can forget about the probability measure, definition in terms of probability measures, and just think about what $F_N$ satisfies this equation, and what happens when $N$ becomes large. It also suggests that as $N\to \iy$, the RHS will vanish.

I think of this as an evolution equation; think of $t$ as time. It will be useful to understand what happens when $t=0$, the initial conditions.
\begin{align}
F_N(0,h) &= \rc N \log \sum_\si \sum_\si \exp\pa{h \sum_i \si_i}\\
&= \rc N \log \sum_\si \prodo iN \exp(h\si_i)\\
&= \rc N \log \pa{e^h+e^{-h}}^N = \log(e^h+e^{-h})\\
F_N(0,h) &= F_1(0,h) =: \psi(h).
\label{e:pde2}
\end{align}•
This does not depend on $N$.
%close the equation, allow us to completely understand the problem.

The most important connection %between $F_N$ 
is that the $h$-derivative is the mean magnetization~\eqref{e:pd-h}.

What do we do with~\eqref{e:pde1} and~\eqref{e:pde2}?  

\subsection{Interlude on Hamilton-Jacobi equation}

Let's take a step back and think about what the equation is saying. We need to think about what it means to be a solution of 
\begin{align}
\label{e:hj}
\pl_t f - (\pl_h f)^2&=0.
\end{align}•
The first thing to look for is a $C^1$ function that solves the equation pointwise. What's the problem with this?

The problem is that there is a phase transition in the Ising model. When $t$ is small nothing impressive happens. For fixed small $t$, $F_N(t,h)$ will be smooth. 

But for larger $t$, if $h$ is positive, then the mean magnetization is positive and away from 0, and if $h$ is tiny negative, then the mean magnetization is negative and away from 0. There will be a jump in the derivative of the function; it looks like $|h|$. The equation is not solved pointwise at $h=0$.

QA:
\begin{itemize}
\item
If you change the measure, you can create lots of discontinuities.
Considering $P$ with bounded support on $\R$, we can consider
\begin{align*}
\int \exp(\cdots) \,dP^{\ot N}(\si). 
\end{align*}•
You can play with $P$ to create more corners in the limit.
This changes what this $\psi$ function.
%$\pl_t f- |\nb f|^2=0$
\item 
What's the notion of convergence for solutions as $N\to \iy$? All functions are uniformly Lipschitz. By Arzela-Ascoli there are convergent subsequences. We can take uniform convergence as the topology. If you prove convergence for some topology, you can bring it to $C^{0,1}$ topology.
\item 
HJ equation can be solved by characteristics. Is there a probabilistic interpretation of the PDE method of characteristics?
I'll try to bypass it. 
%Lelarge, Miolane 
% 11:40
%Lelarge-Miolane: https://arxiv.org/abs/1611.03888
%Barbier-Macris: https://arxiv.org/abs/1705.02780
Barbier and Macris \cite{barbier2019adaptive} 
use different techniques: construct characteristics for finite $N$.
The method I present is more convenient, you don't need to follow characteristics closely, just look at whether characteristics are contracting or expanding. % inward or outward pointing.
\end{itemize}•

\subsection*{2020/5/19 Lecture 2}

Recall that we showed $\pl_t F_N - (\pl_h F_N)^2 = \rc N \pl_h^2 F_N$ \eqref{e:pde1}, and hoped the limit object will solve the equation~\eqref{e:hj},
\begin{align*}
\pl_t f- (\pl_h f)^2 &=0.
\end{align*}
We have to think about what it means to solve the equation. Because of phase transitions, we don't expect solutions to be $C^1$.
We need to lower our expectations about what being a solution means.

Every Lipschitz function is differentiable almost everywhere. What if we just ask the equation to be satisfied almost everywhere?
The problem is that the solution is not unique (given the initial value at $t=0$, there are multiple possible solutions). Here are some examples.
\begin{itemize}
\item
0 is a solution.
\item
$(t,h)\mapsto t+h$ or $t-h$.
\item
From these 3 solutions, we can construct another solution with the ``tent function,"
\begin{align*}
f(t,h) &=\begin{cases}
t+h,&-t\le h\le 0\\
t-h,&0\le h\le t\\
0,&\text{else.}
\end{cases}•
\end{align*}•
We have an infinite number of solutions by taking combinations of translations of these.
\end{itemize}
But these are not the solutions we care about! We know the solution should be convex in the $h$ variable; the tent function is not convex. If we add in this condition, then we get a unique solution.
\begin{df}
We say that a Lipschitz function $\R_+\times \R\to \R$ is a \vocab{weak solution} of the HJ equation~\eqref{e:hj} if
\begin{enumerate}
\item
(HJ) is satisfied almost everywhere.
\item
For every $t\ge 0$, the mapping $h\mapsto f(t,h)$ is convex.\footnote{It suffices to be semi-convex: there is a lower bound on the Hessian.}
\end{enumerate}•
\end{df}
\begin{pr}[Uniqueness]
If $f$ and $g$ are two weak solutions of the HJ equation with $f(0,\cdot)=g(0,\cdot)$, then $f=g$.
\end{pr}
Why do we want this?
This proposition says that two solutions with same initial condition are equal. What we want is actually a refinement of the statement: we want to compare finite-$N$ solution (to \eqref{e:pde1}) for $N$ large to the solution of the HJ equation.
 % $N$ is large, want to compare to . Refinement of statement. 
Instead of two solutions being equal, we want to show the almost-solution and the solution are close. 
%This is a simplification. 

\begin{proof}[Proof sketch.]
Denote $w=f-g$. We have that almost everywhere
\begin{align*}
\pl_t w&= (\pl_hf)^2 - (\pl_hg)^2\\
&= \ub{(\pl_h f+\pl_hg)}{=:b} \pl_h w.
\end{align*}•
Then 
\begin{align*}
\pl_t w - b\pl_h w&=0\text{ (a.e.)}.
\end{align*}•
Because $f,g$ are convex in $h$, the derivative of $b$ is positive.\footnote{For higher dimensions, the divergence of this vector field has a fixed sign, which says that the flows of the vector field being convergent or divergent.}
The rough idea is to look at how the integral $I(t)$ evolves:
\begin{align*}
I(t) &= \int w(t,h)\,dh.
\end{align*}•
%worry its infinite
Suppose this is well-defined. Then integrating by parts (suppose there are no boundary terms)
\begin{align*}
\pl_t I(t) = \int\pl_t w &= \int b \pl_hw\\
&=-\int \ub{\pl_h b}{\ge0} w
\end{align*}•
%if f above b, it will become smaller. If negative, go back to 0.
This says that $I(t)$ wants to come back to 0.
Some problems with this proof:
\begin{enumerate}
\item
We pretended $w$ has a fixed sign.
\item We integrated over whole space. This is a problem because of boundary terms when integrating by parts.
\end{enumerate}•

Now let's be more rigorous. Let $\phi(x) = \fc{x^2}{1+x^2}$, $v=\phi(w)=\phi(f-g)$. Showing $w=0$ is equivalent to showing $v=0$. We have by the chain rule that %chain rule?
\begin{align*}
\pl_t v - b\pl_h v&=0.
\end{align*}•
Now $v$ has a fixed (positive) sign. This solves item 1.

Denote $L=\ve{\pl_h f}_{L^\iy}+\ve{\pl_h g}_{L^\iy} + 1$. 
Fix $T<\iy$, and study
%sign of interval we integrate over shifts. fix finite time.
\begin{align*}
J(t) :&= \int_{-L(T-t)}^{L(T-t)} v(t,h)\,dh\\
&= \int_{-R_t}^{R_t} v
\end{align*}•
Note $J$ is Lipschitz in $t$, so it has a derivative a.e. By the Fundamental Theorem of Calculus,
\begin{align*}
\pl_t J &=\int_{-R_t}^{R_t} \pl_t v - L(v(t,R_t)+v(t,-R_t))\\
\int_{-R_t}^{R_t} \pl_t v=\int_{-R_t}^{R_t} b \pl_h v
&=-\int_{-R_t}^{R_t} (\pl_h b)v + [bv]_{-R_t}^{R_t}.
\end{align*}•
%tendency to let $J$ increase
The term $[bv]^{-R_t}_{R_t}$ may be positive. %; have a tendency to let $J$ increase. 
However, it will be compensated by the $-$ terms because $|b|\le L$.
\begin{align*}
\pl_t J &\le -\int_{-R_t}^{R_t} \ub{(\pl_h b)}{\ge 0}\ub{v}{\ge 0}\le 0\\
\pl_t J&\le 0.
\end{align*}
We know $J(0)=0$ and $J\ge 0$. We conclude that $J\equiv 0$, and $v=0$ a.e. Because $v$ is Lipschitz, $v=0$ and $f=g$.
\end{proof}
To be fully rigorous, we should justify interchanging the derivative and integral, and differentiating $b$ (since $f,g$ are only Lipschitz, and may not be twice differentiable).
\begin{exr}[for credit]
Make this proof rigorous.

Hint: convolve with a function to make it smooth. See the book \cite{evans2010partial}.
\end{exr}
%convexity preserved under pointwise limits. f_N convex for each N, limit f should be convex.
What makes the proof work is that the nonlinearity $H$ is convex:
\begin{align*}
\pl_t f - H(\nb f)&=0.
\end{align*}•
Difficulty comes in when $H$ is not convex or concave.
%non-convex Legendre transform in stat mech. these indicate non-stability or phase coexistence. There are cases where the free energy is not convex. There is a more general notion of Legendre transform, which also applies to non-convex functions.
%non-str convex funs, still have to be cvs

Any limit point of a solution should be a solution: You can try to run the proof, show the sequence of functions is a Cauchy sequence, and the limit is a solution.

We can write down a formula for the solution.
\begin{pr}[Hopf-Lax formula]
Let $\psi$ be convex and Lipschitz.\footnote{It suffices to be locally semi-convex: for every $\de>0$, there exists $C_\de<\iy$ such that for all $\ge \de$, $h\mapsto f(t,h) + C_\de h^2$.
(Note the lower bound is degenerate as $t\to 0$.)
More generally, this is true under mild regularity assumptions, though we have to define weak solution differently.} 
The function
\begin{align*}
f(t,h) &= \sup_{h'\in \R} \pa{\psi(h-h') - \fc{(h')^2}{4t}}
\end{align*}•
%cvx lip
is the weak solution of
\begin{align*}
\pl_t f - \pa{\pl_h f}^2 &=0\\
f(0,\cdot)&=\psi.
\end{align*}•
%convex and Lipschitz. For any regularity, continuous. Convexity assumption, reword weak solution
\end{pr}
\footnote{Is there a modification for the pre-limit PDE in $F_N$? There would be some Brownian motion. $f$ will be the some expecation of some exponential of Brownian motion. For our application, we will not have a completely closed formula for $F_N$, so we cannot write a formula of this form before passing to the limit.}
Note the last term is related to the convex dual of the nonlinearity: the convex dual of $p\mapsto p^2$ is $q\mapsto \fc{q^2}4$. See~\cite{evans2010partial}.
\begin{exr}
Suppose that $f$ solves $\pl_t f- (\pl_hf)^2=0$ and $f(0,h)=\psi(h) = \log(e^h+e^{-h})$. 

Show that for $t>0$ small that $\pl_h f(t,0) = 0$. You can use the Hopf-Lax formula.

For $t<\iy$ large, $\pl_h^+f(t,0)>0> \pl_h^-f(t,0)$; there is a phase transition in the derivative. %deriv on positive side
\end{exr}

\subsection{Back to Curie-Weiss}
%It's not necessarily true that 
In general it's not true that convergence of functions implies convergence of their derivatives, so we need to prove the following.
\begin{pr}
If $(t,h)$ is a point of differentiability (in $h$) of $f$, and if $F_N\to f$ (pointwise), then 
\begin{align*}
\pl_h F_N(t,h) &\to \pl_h f(t,h).
\end{align*}
\end{pr}
\begin{proof}
$F_N$ is convex in $h$, so
\begin{align*}
F_N(t,h') &\ge  F_N(t,h) +\ub{\pl_h F_N(t,h)}{\text{bounded}}(h'-h).
\end{align*}
Because the derivatives are bounded, we can take a subsequence along which $\pl_h F_N(t,h)\to p$.
Then
\begin{align*}
f(t,h') &\ge f(t,h) + p(h'-h), 
\end{align*}•
and $p$ must be $\pl_h f(t,h)$.
%fully rigorous
\end{proof}
\paragraph{Convergence of $F_N$.}
%perturb of uniqueness arg
We have
\begin{align*}
\pl_t F_N - (\pl_h F_N)^2 &= \rc N \pl_h^2 F_N.
\end{align*}•
We want $F_N$ to be close to the HJ solution. Let $w=F_N-f$; then
\begin{align*}
\pl_t w - b \pl_h w &= \rc N \pl_h^2 F_N\\
\text{where } b&= \pl_h F_N + \pl_h f.
\end{align*}•
Let $v=\phi(w)$. 
Before, the difference solved the same equation, but now we have to be more careful. 
\begin{align*}
\pl_t v - b \pl_h v &= \phi'(w) \rc{N}\pl_h^2F_N.
\end{align*}•
The argument is similar, but the RHS is different here. Define
\begin{align*}
J(t) &=\int_{-R_t}^{R_t} v(t,h)\,dh.
\end{align*}•
We have an extra term
\begin{align*}
\pl_t J &\le \ub{\int_{-R_t}^{R_t} b\pl_h v -L(v(t,R_t)+v(t,-R_t))}{\le 0}+ \int_{-R_t}^{R_t}\ub{\phi'(w)}{\le 1} \rc N \ub{\pl_h^2 F_N(t,h)}{\ge0}\,dh \\
\pl_t J &\le \rc N \int_{-R_t}^{R_t} \pl_h^2 F_N(t,h) \,dh
=\rc{N} [\pl_h F_N]_{-R_t}^{R_t} \le \fc 2N.
\end{align*}
%don't need sign
%because this is the integral of a derivative.
where $|\pl_h F_N|\le 1$ follows from~\eqref{e:pd-h}. 
Recall $J(0)=0$. So $J(t)\le \fc{2t}{N}$.
%close in L1, L\iy
\begin{exr}
Clean up this proof! How do you get pointwise convergence?
\end{exr}

In the rank-1 case, we find some quantities similar to $F_N$. We have a similar situation with a function which satisfies a similar equation, and want to show it converges to the true solution. We need some $L^1$ estimate.

\paragraph{Key point:}
%The key point is the following.
If there are ``error terms on the right hand side," we need to estimate it in $L^1$ in the $h$ variable uniformly in $h$ (locally). 
We want a ``local $L_t^\iy,L_h^1$ estimate." 

\subsection*{2020/5/21 Lecture 3}

\subsection{Matrix estimation problem}

We saw how to justify the solution in the limit for the Curie-Weiss model, showing error term disappears. Now we consider the matrix estimation problem. This has already been solved by other people. The first that gave a complete solution of this problem is~\cite{lelarge2019fundamental}. Shortly after,~\cite{barbier2019adaptive} gave another proof. 

We consider the problem in~\cite{mourrat2018hamilton} using the approach in~\cite{mourrat2019hamilton}.

%iid

Problem: Let $\ol x = (\ol x_1,\ldots, \ol x_N)$ be a vector of bounded independent random variables with law $P_N=P^{\ot N}$. For some $t>0$, we observe
\begin{align*}
Y &= \sfc{2t}N \ol x \ol x^\top +W
\end{align*}•
where $W=(W_{ij})$ are independent standard Gaussians. (We don't assume symmetry.)

%We observe matrix $Y$.
%I don't assume any symmetry.
%I'll assume we can find the best possible estimator given the data.

For example, we want to understand
\begin{align*}
\text{mmse}_N(t) &= \rc{N^2}  \inf_{\wh \te} \E\ba{|\ol x \ol x^\top - \wh \te (Y)|^2}\\
& = \rc{N^2} \E \ba{|\ol x\ol x^\top - \E[\ol x\ol x^\top |Y]|^2}.
\end{align*}•
where infimum over all estimators. We assume we can find the best possible estimator given the data, which is the conditional expectation given $Y$.
%%look for best $\wh \te$ that minimizes, it's the conditional expectation.
%projecting into L2 space.

Informally, 
\begin{align*}
\Pj\ba{\ol x=x \text{ and }Y=y}
&= dP_N(x)\,dy\, \exp\pa{-\rc 2 \ab{Y-\sfc{2t}{N} x x^\top }^2}
\end{align*}•
Now compute the conditional law,
\begin{align*}
\Pj[\ol x= x|Y] &= \fc{\exp\pa{-\rc 2 \ab{Y-\sfc{2t}{N}x x^\top }^2}\,dP_N(x)}{\int \exp\pa{-\rc 2 \ab{Y-\sfc{2t}{N}x' x^{\prime\top} }^2}\,dP_N(x')}
\end{align*}•
Expand the exponent and remove $-\rc 2|Y|^2$ (which doesn't depend on $t,x$) to obtain the following.
\begin{df}
Define
\begin{align*}
H_N^\circ (t,x) &= \sfc{2t}N Y \cdot xx^\top - \fc tN |x|^4.
\end{align*}•
\end{df}
Here $Y$ is the sum of 2 terms, $\sfc{2t}N \ol x \ol x^\top +W$. Substituting and expanding gives
\begin{align*}
H_N^\circ (t,x) &= \sfc{2t}{N} x\cdot Wx + \fc{2t}N(x\cdot \ol x)^2 - \fc tN |x|^4.
\end{align*}•
The first term is the important term; it is like the spin glass model. 
The other terms will help us. They aren't negligible, but they are here to make out life easier. 
In summary, the conditional law of $\ol x|Y$ has the form of a Gibbs measure, with the most important part looking like the spin glass case.

Denote 
\begin{align*}
\an{f(x)} &= \fc{\int f(x) \exp(H_N^\circ (t,x))\,dP_N(x)}{\int \exp(H_N^\circ (t,x))\,dP_N(x)}.
\end{align*}•
The important difference with the Curie-Weiss model is that $H_N^\circ (t,x)$ is still random. 

\begin{exr}[for credit]
Make this rigorous: Show that
\begin{align*}
\an{f(x)} &= \E[f(\ol x)|Y].
\end{align*}•
\end{exr}

Why is this model, which looks like the spin glass model, simpler than the spin glass model? The important property is the following.
\begin{pr}[Nishimori]
\begin{align*}
\E\an{f(x)} &= \E[f(\ol x)]\\
\E\an{f(x)g(x')} &= \E[\an{f(x)}\an{g(x)}]\\
&=\E[\an{f(x)}\E[g(\ol x)|Y]]\\
%insert into conditional expecation and remove conditioning
&= \E\an{f(x) g(\ol x)}.
\end{align*}•
where $x'$ is an independent copy of $x$ under $\an{\cdot}$.
More generally,
\begin{align*}
\E\an{f(x,x')} &= \E\an{f(x,\ol x)}
\end{align*}•
\end{pr}
What is $x$ and why is it different from $\ol x$? Some intuition: Suppose that if we look at $Y$, we learn nothing about $\ol x$; then $x$ is a resample. If $Y$ reveals perfect information on $\ol x$, then $x=\ol x$. The general case is like an interpolation between these trivial cases.

The independent copy $x'$ is called a \vocab{replica}.

This is not trivial, it's not saying $x=\ol x$. It is only true because I average outside. 

This will help us identify the partial differential equation; we can link it back to the original signal. 

%parameter that allows you to say something about strength of signal. If signal-to-noise too small, can't recover.

We will identify the PDE and compute the mmse by looking at the derivative in the limit, you can recover the phase transition.

Here $\ol x$ is the original random vector, and $x$ is our best guess for $\ol x$ given what we've seen. 

%study, apply same strategy
We want to take the log and look for a PDE. But I only have $t$ in this function. If I describe the Curie-Weiss model and had not thought of adding the $h$; then I would be stuck, I can differentiate in $t$ many times and not find a equation. The relation was between derivatives in $t$ and $h$. I have $t$, but what do I relate it to? We play the same game, add a $h$ parameter. Sufficient to relate derivatives.

We have a quadratic term; I would like to add a linear term in $x$ in the model. We want to do this with a constraint: I don't want to destroy the Nishimori property. For Curie-Weiss this was easy, but for spin glasses it's subtle, not easy to figure out in general.
%conditional expectation structure.

\emph{We need to enrich the model somehow!}

It has to be rich enough to relate derivatives, and simple enough to compute what happens. Suppose we also observe, %linear observation on $\ol x$
\begin{align*}
\sqrt{2h}\ol x + z
\end{align*}•
where $z$ is a standard Gaussian vector. 
%W, z noise terms
Now I recompute the conditional law of $\ol x$ given this rich observation,
\begin{align*}
H_N(t,h,x) &= H_N^\circ (t,x) + \sqrt{2h} x\cdot z  + 2h x\cdot \ol x - h|x|^2.
\end{align*}•
The important part is the term $\sqrt{2h} x\cdot z$.
%CW with random magnetic field with gaussians.

Now I can define the free energy,
\begin{align*}
F_N(t,h) &= \rc N \E\log \int \exp(H_N(t,h,x)) \,dP_N(x)\\
\ol F_N(t,h) &= \E F_N(t,h) %\rc N \E\log \int \exp(H_N(t,h,x)) \,dP_N(x).
\end{align*}•
%normalizing constant, or mgf. 
I take the expectation because the expression inside depends on the randomness in the noise.
\begin{thm}
$\ol F_N\to f$ (local uniform convergence) which is a solution of 
\begin{align*}
\pl_t f - (\pl_h f)^2 &= 0&\text{on }\R_+\times \R_+\\
f(0,\cdot) &= F_1(0,\cdot)
\end{align*}•
\end{thm}
%only nonneg
%local uniform convergence
%uniformly LIp
Note $\ol F_N(0,\cdot)$ does not depend on $N$.

\subsection{Main steps}

The proof has two parts. First we relate the derivates to a variance.
\begin{pr}
\begin{align*}
\pl_t \ol F_N - (\pl_h\ol F_N)^2 &= \rc{N^2} \E\an{(x\cdot \ol x - \E\an{x\cdot \ol x})^2}.
\end{align*}•
\end{pr}
Next, we find a way to express this variance in terms of other things. In the Curie-Weiss model we also described this in terms of derivatives. Here we don't have an equality.
\begin{pr}\label{p:me2}
We have
\begin{align*}
\E\an{(x\cdot \ol x - \E\an{x\cdot \ol x})^2}
&\le \rc N \pl_h^2 \ol F_N + \E[(\pl_h F_N- \pl_h \ol F_N)^2]
\end{align*}•
\end{pr}
The last term is still random, so we have to control it.

I will justify why this is true, and how to conclude the main result.
For the first part, we use the following.
\begin{lem}\label{l:me1}
\begin{align*}
\pl_h \ol F_N &= \E\an{\fc{x\cdot \ol x}{N}}\\
\pl_t \ol F_N &= \E\an{\pf{x\cdot\ol x}N^2}.
\end{align*}•
\end{lem}

The key ingredient is It\^o calculus without It\^o. 
Letting $X$ be a standard gaussian, 
\begin{align*}
\E[\exp(\sqrt{2t}X-t)]&=1.
\end{align*}•
(Taking the expectation of Brownian motion and subtracting $t$, we get a constant. Here $\sqrt{2t}X$ is like $B_{2t}$.)
Differentiating with respect to $t$ we should get 0.
\begin{align*}
\E\ba{
\pa{
\rc{\sqrt{2t}}X-1
}\exp(\sqrt{2t}X-t)
}&=0.
\end{align*}•
How to see this without It\^o calculus? It's something about gaussians. $x\exp(-\fc{x^2}2)$ want to come together and be integrated out. %only thing I can do is IbP
Using integration by parts,
\begin{align*}
\int x \exp(\sqrt{2t}x-t) \exp\pa{-\fc{x^2}2}\,dx
&= \int \sqrt{2t}\exp(\sqrt{2t}x-t) \exp\pa{-\fc{x^2}2}\,dx.
\end{align*}
This is Gaussian integration by parts.
\begin{exr}[for credit]
\begin{enumerate}
\item
Prove Lemma~\ref{l:me1} using Gaussian integration by parts and Nishimori.
\item
Relate mmse with $\pl_t \ol F_N$.
\end{enumerate}
\end{exr}
For Proposition~\ref{p:me2}, letting $A$ be a random variable, notice that 
\begin{align*}
\E\an{(A-\E\an{A})^2}
&= \E\an{(A-\an{A})^2} 
+ \E[(\an{A}-\E\an{A})^2].
\end{align*}
%like orthogonality of L_2 martingale increments.
We do expectation one step at a time.
The first term looks like Curie-Weiss, and produces $\rc N \pl_h^2 \ol F_N$. The second term is new, and produces $\pl_h F_N - \pl_h\ol F_N$.

The new ingredient is a classical concentration estimate. The Efron-Stein %gaussian conc
inequality shows $\E[|F_N-\ol F_N|^2]$ is small.
%Because we are allowed to do this averaging, 

Phase transition in $t$: For small $t$, we have $\E[\ol x_1]=0$, $\pl_t f=0$; then it detaches itself from 0, $\pl_t f>0$.
%understand mmse
Before the transition, you have maximal error; then the error reduces.

We've talked about best estimation when there are unlimited resources? People believe there is another regime where it is possible but hard to learn something about $\ol x$.

Convergence rate: In the Curie-Weiss, we got the rate for free. In this case, if you follow through, you can get rates. In the paper~\cite{mourrat2019hamilton} I was not careful. %Main estimate I gave here, error term in paper. 
It's not straightforward to reconstruct the argument in this talk for paper. In the finite-rank case it's not clear that $\ol F_N$ is convex. I only know lower bound on convexity, %, lower-bounded by constant, 
which blows up when $h\to 0$. In the rank-1 case it's convex. From the argument here ,you can get good, $N^{-a}$ convergence (check integration by parts). Certainly you can get $N^{-1/4}$ without a problem. 

For $p$ spin, we consider $\pl_t f - (\pl_h f)^p$ instead. Perhaps you can get $|\pl_t f-(\pl_h f)^3|\le \fc{C}{N} (\pl_h^2 f)^{1/2}$. 
%viscocity solutions and maximal principle, you don't want to flip the sign of the Laplacian, but you know in advance the function is convex, you will not create in the wrong side.

%p-spin
%under which conditions is it possible to map a lattice model into a stochastic process through HJ equations. Is it possible to see Mckean-Vlasov equations as a generalization of this mapping?

%spin-glass context. algorithm, finds configuration, as high as possible, maximum. 
%Eliran Subag, up to some level. 
%LM. 

